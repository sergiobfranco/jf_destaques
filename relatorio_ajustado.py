# Instalar as bibliotecas necess√°rias no Google Colab
#!pip install python-docx

import re
import os
from docx import Document
from docx.shared import RGBColor
from docx.oxml.shared import OxmlElement, qn
#from config import arq_resumo_final_ajustado, arq_resumo_final
from datetime import datetime

def adicionar_hyperlink(paragraph, url, texto_display):
    """
    Adiciona um hyperlink a um par√°grafo no documento Word
    """
    # Criar o elemento hyperlink
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), paragraph.part.relate_to(url,
                                                       "http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink",
                                                       is_external=True))

    # Criar o run com o texto do link
    new_run = OxmlElement('w:r')

    # Configurar propriedades do texto (cor azul, sublinhado)
    rPr = OxmlElement('w:rPr')

    # Cor azul
    color = OxmlElement('w:color')
    color.set(qn('w:val'), '0000FF')
    rPr.append(color)

    # Sublinhado
    underline = OxmlElement('w:u')
    underline.set(qn('w:val'), 'single')
    rPr.append(underline)

    new_run.append(rPr)

    # Adicionar o texto
    new_run.text = texto_display
    hyperlink.append(new_run)

    return hyperlink

def processar_urls_em_paragrafo(paragraph):
    """
    Processa um par√°grafo, convertendo URLs em hyperlinks
    """
    texto_completo = paragraph.text

    # REGEX CORRIGIDA - Captura URLs completas incluindo par√¢metros, fragmentos, etc.
    # Esta regex √© mais robusta e captura URLs at√© encontrar espa√ßo ou fim de linha
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    # Alternativa ainda mais simples e eficaz:
    # padrao_url = r'https?://\S+'

    urls_encontradas = re.findall(padrao_url, texto_completo)

    # Remover URLs duplicadas mantendo a ordem
    urls_unicas = []
    for url in urls_encontradas:
        if url not in urls_unicas:
            urls_unicas.append(url)

    if urls_unicas:
        print(f"   üîó Encontradas {len(urls_unicas)} URLs: {urls_unicas[:2]}{'...' if len(urls_unicas) > 2 else ''}")

        # Limpar o par√°grafo atual
        paragraph.clear()

        # Dividir o texto pelas URLs
        texto_restante = texto_completo

        for url in urls_unicas:
            # Encontrar a posi√ß√£o da URL no texto
            if url in texto_restante:
                partes = texto_restante.split(url, 1)

                if len(partes) == 2:
                    # Adicionar texto antes da URL
                    if partes[0]:
                        paragraph.add_run(partes[0])

                    # Adicionar hyperlink
                    hyperlink_element = adicionar_hyperlink(paragraph, url, url)
                    paragraph._p.append(hyperlink_element)

                    # Continuar com o resto do texto
                    texto_restante = partes[1]

        # Adicionar texto restante ap√≥s a √∫ltima URL
        if texto_restante:
            paragraph.add_run(texto_restante)

        return True

    return False

def converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida):
    """
    Converte URLs em hyperlinks em um arquivo DOCX
    """
    try:
        # Abrir o documento
        print(f"üìñ Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        urls_convertidas = 0
        paragrafos_processados = 0
        paragrafos_com_urls = 0

        # Processar todos os par√°grafos
        print("üîÑ Processando par√°grafos...")
        for i, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():  # Ignorar par√°grafos vazios
                if processar_urls_em_paragrafo(paragraph):
                    paragrafos_com_urls += 1
                paragrafos_processados += 1

                # Mostrar progresso a cada 25 par√°grafos
                if paragrafos_processados % 25 == 0:
                    print(f"   üìÑ Processados {paragrafos_processados} par√°grafos ({paragrafos_com_urls} com URLs)...")

        # Processar tabelas (se houver)
        print("üìä Processando tabelas...")
        tabelas_processadas = 0
        celulas_com_urls = 0

        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if paragraph.text.strip():
                            if processar_urls_em_paragrafo(paragraph):
                                celulas_com_urls += 1
            tabelas_processadas += 1

        # Salvar o documento processado
        print(f"üíæ Salvando arquivo: {arquivo_saida}")
        doc.save(arquivo_saida)

        print(f"\n‚úÖ Convers√£o conclu√≠da com sucesso!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Par√°grafos processados: {paragrafos_processados}")
        print(f"   - Par√°grafos com URLs: {paragrafos_com_urls}")
        print(f"   - Tabelas processadas: {tabelas_processadas}")
        print(f"   - C√©lulas com URLs: {celulas_com_urls}")
        print(f"   - Arquivo salvo como: {arquivo_saida}")

        return True

    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo '{arquivo_entrada}' n√£o encontrado!")
        print("Verifique se o arquivo existe no diret√≥rio atual.")
        return False

    except Exception as e:
        print(f"‚ùå Erro durante o processamento: {str(e)}")
        print(f"Detalhes do erro: {type(e).__name__}")
        return False

def metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida):
    """
    M√©todo alternativo melhorado - substitui URLs por texto com formata√ß√£o
    """
    try:
        print(f"üìñ M√©todo alternativo melhorado - Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        total_urls_encontradas = 0
        paragrafos_processados = 0

        # Regex melhorada para capturar URLs completas
        padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

        # Processar par√°grafos
        for paragraph in doc.paragraphs:
            if not paragraph.text.strip():
                continue

            texto_original = paragraph.text
            urls_no_texto = re.findall(padrao_url, texto_original)

            if urls_no_texto:
                print(f"   üîó Par√°grafo {paragrafos_processados + 1}: {len(urls_no_texto)} URLs encontradas")
                total_urls_encontradas += len(urls_no_texto)

                # Limpar o par√°grafo
                paragraph.clear()

                # Reconstruir o par√°grafo com formata√ß√£o
                texto_restante = texto_original

                for url in urls_no_texto:
                    if url in texto_restante:
                        # Dividir o texto pela URL
                        partes = texto_restante.split(url, 1)
                        if len(partes) == 2:
                            # Adicionar texto antes da URL
                            if partes[0]:
                                paragraph.add_run(partes[0])

                            # Adicionar URL com formata√ß√£o especial
                            run_url = paragraph.add_run(url)
                            run_url.font.color.rgb = RGBColor(0, 0, 255)  # Azul
                            run_url.underline = True

                            # Continuar com o resto
                            texto_restante = partes[1]

                # Adicionar texto restante
                if texto_restante:
                    paragraph.add_run(texto_restante)

            paragrafos_processados += 1

        # Processar tabelas tamb√©m
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if not paragraph.text.strip():
                            continue

                        texto_original = paragraph.text
                        urls_no_texto = re.findall(padrao_url, texto_original)

                        if urls_no_texto:
                            total_urls_encontradas += len(urls_no_texto)
                            paragraph.clear()

                            texto_restante = texto_original
                            for url in urls_no_texto:
                                if url in texto_restante:
                                    partes = texto_restante.split(url, 1)
                                    if len(partes) == 2:
                                        if partes[0]:
                                            paragraph.add_run(partes[0])

                                        run_url = paragraph.add_run(url)
                                        run_url.font.color.rgb = RGBColor(0, 0, 255)
                                        run_url.underline = True

                                        texto_restante = partes[1]

                            if texto_restante:
                                paragraph.add_run(texto_restante)

        # Salvar documento
        doc.save(arquivo_saida)

        print(f"\n‚úÖ M√©todo alternativo conclu√≠do!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Par√°grafos processados: {paragrafos_processados}")
        print(f"   - Total de URLs formatadas: {total_urls_encontradas}")
        print(f"üíæ Arquivo salvo como: {arquivo_saida}")

        return True

    except Exception as e:
        print(f"‚ùå Erro no m√©todo alternativo: {str(e)}")
        return False

def testar_regex():
    """
    Fun√ß√£o para testar a regex com URLs de exemplo
    """
    print("üß™ Testando regex com URLs de exemplo...")

    # URLs de teste
    urls_teste = [
        "https://tinyurl.com/2aymnjlf",
        "https://www.google.com/search?q=python",
        "http://example.com/path/to/file.html",
        "https://github.com/user/repo#readme",
        "https://site.com/page?param1=value1&param2=value2"
    ]

    # Regex melhorada
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    for url in urls_teste:
        match = re.search(padrao_url, url)
        if match:
            print(f"   ‚úÖ {url} -> Capturado: {match.group()}")
        else:
            print(f"   ‚ùå {url} -> N√£o capturado")

    print("\n" + "="*50)

def gerar_versao_ajustada(arquivo_entrada):
    """
    Fun√ß√£o principal para executar a convers√£o
    """
    # Nomes dos arquivos (ajuste conforme necess√°rio)
    #arquivo_entrada = arq_resumo_final  # Seu arquivo original

    # Gerar novo nome de arquivo com timestamp antes do sufixo .docx
    base, ext = os.path.splitext(arquivo_entrada)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    arq_resumo_final_ajustado = f"{base}_{timestamp}{ext}"
 
    arquivo_saida = arq_resumo_final_ajustado  # Arquivo com hyperlinks

    print("üöÄ Iniciando convers√£o de URLs para hyperlinks...")
    print("=" * 60)

    # Testar regex primeiro
    testar_regex()

    # Tentar primeiro m√©todo (hyperlinks verdadeiros)
    print("\nüîó Tentando m√©todo com hyperlinks verdadeiros...")
    sucesso = converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida)

    if not sucesso:
        print("\nüîÑ Tentando m√©todo alternativo melhorado...")
        print("-" * 50)
        arquivo_saida_alt = "arq_resumo_final_formatado.docx"
        sucesso = metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida_alt)

    if sucesso:
        print("\nüéâ Processo finalizado com sucesso!")
        print("\nüí° Dicas:")
        print("   - Abra o arquivo no Word para verificar os hyperlinks")
        print("   - Os links devem estar em azul e sublinhados")
        print("   - Teste alguns links clicando neles")
        print("   - Se algum link ainda estiver truncado, execute novamente")
    else:
        print("\n‚ùå N√£o foi poss√≠vel completar a convers√£o.")
        print("Verifique se o arquivo de entrada existe e n√£o est√° corrompido.")

