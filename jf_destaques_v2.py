# -*- coding: utf-8 -*-
"""JF_Destaques_V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uaqkZyBoRpx1boqI5EI6WmLN4bDgEdum

#**Destaques do Dia J&F**

"""
import os

# Nova defini√ß√£o do caminho base
JF_path = os.path.dirname(__file__)

#Temporizador

import datetime
import time
import pytz

# Fun√ß√£o para solicitar e validar data e hora
def solicitar_data_hora_futura():
    while True:
        entrada = input("Digite a data e hora futuras no formato AAAA-MM-DD HH:MM:SS: ")
        try:
            data_hora_futura = datetime.datetime.strptime(entrada, "%Y-%m-%d %H:%M:%S")

            # Adicionar fuso hor√°rio de S√£o Paulo
            fuso_horario_sp = pytz.timezone('America/Sao_Paulo')
            data_hora_futura_sp = fuso_horario_sp.localize(data_hora_futura)

            # Obter data e hora atual em SP
            data_hora_atual_sp = datetime.datetime.now(fuso_horario_sp)
            diferenca_tempo = data_hora_futura_sp - data_hora_atual_sp
            segundos_para_esperar = diferenca_tempo.total_seconds()

            if segundos_para_esperar < 0:
                print("A data e hora futuras especificadas j√° passaram.")
                continuar = input("Deseja continuar mesmo assim? (S/N): ").strip().upper()
                if continuar == "S":
                    return segundos_para_esperar
                else:
                    print("Vamos tentar novamente...\n")
                    continue
            else:
                return segundos_para_esperar

        except ValueError:
            print("Formato inv√°lido. Use o formato AAAA-MM-DD HH:MM:SS.\n")

# Chamada da fun√ß√£o
segundos_para_esperar = solicitar_data_hora_futura()

# Aguardar se for no futuro
if segundos_para_esperar > 0:
    horas = segundos_para_esperar / 60 / 60
    print(f"Faltam {horas:.2f} horas at√© a data e hora futuras especificadas em S√£o Paulo.")
    print("Aguardando...")
    time.sleep(segundos_para_esperar)
    print("Espera terminada. O c√≥digo continuar√° a execu√ß√£o.")
else:
    print("Continuando imediatamente pois a data especificada j√° passou.")


"""#In√≠cio do Cron√¥metro"""

import datetime

# In√≠cio do Cron√¥metro
start_time = datetime.datetime.now()
print(f"In√≠cio do processamento: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

"""#**V a r i √° v e i s - G l o b a i s**"""

# Pastas
pasta_api = "dados/api"
pasta_marca_setor = "dados/marca_setor"

# Arquivo de Favoritos gerado pela API - ORIGINAL
favoritos_marca = "Favoritos_Marcas.xlsx"
arq_api_original = os.path.join(pasta_api, favoritos_marca)
aux = "aux.xlsx"
arq_aux = os.path.join(pasta_api, aux)

favoritos_setor = "Favoritos_Setor.xlsx"
arq_api_original_setor = os.path.join(pasta_api, favoritos_setor)

favoritos_setor_inter = "Favoritos_Setor_inter.xlsx"
arq_api_original_setor_inter = os.path.join(pasta_api, favoritos_setor_inter)

favoritos_editorial = "Favoritos_Editorial.xlsx"
arq_api_original_editorial = os.path.join(pasta_api, favoritos_editorial)

favoritos_SPECIALS = "Favoritos_SPECIALS.xlsx"
arq_api_original_SPECIALS = os.path.join(pasta_api, favoritos_SPECIALS)

# Arquivo de Favoritos gerado pela API - SMALL
favoritos_small_marca = "Favoritos_Marcas_small.xlsx"
arq_api = os.path.join(pasta_api, favoritos_small_marca)

favoritos_small_setor = "Favoritos_Setor_small.xlsx"
arq_api_setor = os.path.join(pasta_api, favoritos_small_setor)

favoritos_small_editorial = "Favoritos_Editorial_small.xlsx"
arq_api_editorial = os.path.join(pasta_api, favoritos_small_editorial)

favoritos_small_SPECIALS = "Favoritos_SPECIALS_small.xlsx"
arq_api_SPECIALS = os.path.join(pasta_api, favoritos_small_SPECIALS)

# Arquivo TXT de sa√≠da com os resumos
resumos = "resumos_marcas.txt"
arq_resumos = os.path.join(pasta_marca_setor, resumos)

# Arquivo XLSX de Not√≠cias Similares
similares = "Grupos_Noticias_Similares.xlsx"
arq_similares = os.path.join(pasta_marca_setor, similares)

similares_setor = "Grupos_Noticias_Similares_Setor.xlsx"
arq_similares_setor = os.path.join(pasta_marca_setor, similares_setor)

# Arquivo XLSX de Prompt de Resumo
prompts = "Prompts_Resumo_Noticias.xlsx"
arq_prompts = os.path.join(pasta_marca_setor, prompts)

prompts_setor = "Prompts_Resumo_Noticias_Setor.xlsx"
arq_prompts_setor = os.path.join(pasta_marca_setor, prompts_setor)

# Arquivo XLSX de Resultados
results = "Resumos_Gerados_DeepSeek.xlsx"
arq_results = os.path.join(pasta_marca_setor, results)

results_final = "Resumos_Finais_DeepSeek.xlsx"
arq_results_final = os.path.join(pasta_marca_setor, results_final)


results_setor = "Resumos_Gerados_DeepSeek_Setor.xlsx"
arq_results_setor = os.path.join(pasta_marca_setor, results_setor)

# Arquivo DOCX de Resumos
resumo_final = "Resumo_Marcas.docx"
arq_resumo_final = os.path.join(pasta_marca_setor, resumo_final)

resumo_final_ajustado = "Resumo_Marcas_ajustado.docx"
arq_resumo_final_ajustado = os.path.join(pasta_marca_setor, resumo_final_ajustado)

# Marcas
w_marcas = ['Holding', 'J&F', 'JBS', 'Joesley Batista', 'Wesley Batista', 'J√∫nior Friboi', 'J&F Minera√ß√£o/LHG', 'J&F Minera√ß√£o/LHG Mining', \
            'Banco Original', 'PicPay', 'Eldorado', 'Flora', '√Çmbar Energia', 'Ambar Energia', \
            'Canal Rural', 'Braskem', 'Instituto J&F' ]
marcas_a_ignorar = ['J&F', 'JBS', 'Joesley Batista', 'Wesley Batista', 'J√∫nior Friboi', 'J&F Minera√ß√£o/LHG', 'J&F Minera√ß√£o/LHG Mining', \
                    'Banco Original', 'PicPay', 'Eldorado', 'Flora', '√Çmbar Energia', 'Ambar Energia', \
                    'Canal Rural', 'Braskem', 'Instituto J&F' ]
# Marcas que v√™m em primeiro e segundo lugar no relat√≥rio
marca1 = 'J&F'
marca2 = 'JBS'

"""#Fast Track

# QUEBRA-GALHO
import requests
import pandas as pd
import os
import json
import re

final_df = pd.read_excel(arq_api_original)
final_df_small = pd.read_excel(arq_api)

final_df_SPECIALS_small = pd.read_excel(arq_api_SPECIALS)

final_df_setor = pd.read_excel(arq_api_original_setor)
final_df_setor_small = pd.read_excel(arq_api_setor)
"""

"""#Chamada da API - **MARCAS**"""

# Chamada da API para trazer os favoritos da MVC J&F Gest√£o (eliminando duplicatas de IdVeiculo + Titulo)
import requests
import pandas as pd
import os
import json
import re

# Caminho para o arquivo externo com as configura√ß√µes da API
config_file = os.path.join(JF_path, "dados", "api_marca_configs.json")  # Nome do arquivo de configura√ß√£o

# Carrega as configura√ß√µes da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configura√ß√µes da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se n√£o, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisi√ß√£o POST para a API
    response = requests.post(url, json=data)
    print('response: ', response)
    # Verifique se a requisi√ß√£o foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame √† lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisi√ß√£o para {url}: {response.status_code}")

# Concatena todos os DataFrames em um √∫nico DataFrame
final_df = pd.concat(all_dfs, ignore_index=True)

# Remover duplicatas de IdVeiculo + Titulo
# Convert 'DataVeiculacao' to datetime objects, coercing errors
final_df['DataVeiculacao'] = pd.to_datetime(final_df['DataVeiculacao'], errors='coerce')

# Sort by 'IdVeiculo', 'Titulo', and 'DataVeiculacao' in descending order
final_df = final_df.sort_values(by=['IdVeiculo', 'Titulo', 'DataVeiculacao'], ascending=[True, True, False])

# Remove duplicates based on 'IdVeiculo' and 'Titulo', keeping the first occurrence (which is the latest due to sorting)
final_df = final_df.drop_duplicates(subset=['IdVeiculo', 'Titulo'], keep='first').reset_index(drop=True)


# --- In√≠cio do ajuste ---
print('Qtde de registros antes de desprezar lista de ve√≠culos: ', final_df.shape[0])
# Lista de ve√≠culos a serem ignorados
veiculos_a_ignorar = [
    #"VALOR ECON√îMICO ONLINE/S√ÉO PAULO",
    #"CNN BRASIL ONLINE",
    #"VALOR INVESTE",
    #"ISTO√â DINHEIRO ONLINE/S√ÉO PAULO",
    #"FOLHA DE S.PAULO ONLINE/S√ÉO PAULO",
    #"BLOOMBERG L√çNEA/AM√âRICA LATINA",
    #"R√ÅDIO CBN FM 90,5/S√ÉO PAULO",
    #"O POVO/FORTALEZA",
    #"O POVO ONLINE/FORTALEZA",
    "nenhum"
]

# Filtrar o DataFrame para remover as linhas onde a coluna 'Veiculo'
# est√° presente na lista de ve√≠culos a serem ignorados.
# Garante que a compara√ß√£o seja case-insensitive e remova espa√ßos em branco extras.
final_df = final_df[~final_df['Veiculo'].str.strip().str.upper().isin([v.strip().upper() for v in veiculos_a_ignorar])]

print('Qtde de registros ap√≥s desprezar lista de ve√≠culos: ', final_df.shape[0])

# --- Fim do ajuste ---

# Substitui 'Holding' por 'J&F'

final_df['Canais'] = final_df['Canais'].fillna('').astype(str)
final_df['Canais'] = final_df['Canais'].str.replace(r'\bHolding\b', 'J&F', regex=True)

# Limpeza e ajustes nos Canais

def clean_canais(canais, w_marcas):
  """
  Remove colchetes e aspas, mant√©m apenas marcas em w_marcas,
  e remove v√≠rgulas e espa√ßos extras.
  """
  # 1. Converter para string se n√£o for
  if not isinstance(canais, str):
    canais = str(canais)  # Converte para string

  # 2. Remover colchetes e aspas
  canais = re.sub(r"[\[\]']", "", canais)

  # 3. Manter apenas marcas em w_marcas
  marcas_validas = [marca for marca in canais.split(",") if marca.strip() in w_marcas]

  # 4. Remover v√≠rgulas e espa√ßos extras
  marcas_limpas = [marca.strip() for marca in marcas_validas]

  return ",".join(marcas_limpas)

# Aplicando a fun√ß√£o ao DataFrame
final_df['Canais'] = final_df['Canais'].apply(lambda x: clean_canais(x, w_marcas))

# Replicar registros com v√°rios Canais

# 1. Seleciona apenas as colunas de interesse
final_df_small = final_df[['Id', 'Titulo', 'Conteudo', 'IdVeiculo', 'Canais']].copy()

# 2. Explode os canais em listas
final_df_small['Canais'] = final_df_small['Canais'].str.split(',')

# 3. Replica as linhas
final_df_small = final_df_small.explode('Canais').copy()

# 4. Limpa espa√ßos extras
final_df_small['Canais'] = final_df_small['Canais'].str.strip()

# Grava o DataFrame final em um arquivo Excel

final_df.to_excel(arq_api_original, index=False)
final_df_small.to_excel(arq_api, index=False)

print("Arquivo Excel salvo como ", arq_api_original)
print("Arquivo Excel salvo como ", arq_api)

"""#Avalia√ß√£o de **RELEV√ÇNCIA**"""

# C√ìDIGO ANTERIOR DO PROMPT DE RESUMO ADAPTADO POR MIM PARA
# Vers√£o com avalia√ß√£o de relev√¢ncia da marca usando DeepSeek e agrupamento por similaridade com DBSCAN

import pandas as pd
import os
import requests
from dotenv import load_dotenv

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY n√£o foi encontrada no arquivo .env")

# Carrega vari√°vel do Secrets no Google Colab
#if "DEEPSEEK_API_KEY" not in os.environ:
#    try:
#        load_dotenv()
#       API_KEY = os.getenv("DEEPSEEK_API_KEY")
#        os.environ["DEEPSEEK_API_KEY"] = userdata.get("DEEPSEEK_API_KEY")
#    except Exception as e:
#        raise RuntimeError("Erro ao acessar a chave DEEPSEEK_API_KEY nos Secrets do Colab.") from e

PROMPT_CHARACTER_LIMIT = 30000

arq_api = 'dados/api/Favoritos_Marcas_small.xlsx'
darq_relevancia_irrelevantes = 'dados/api/Favoritos_Marcas_Irrelevantes.xlsx' # Esta vari√°vel n√£o ser√° mais usada para salvar o arquivo
#arq_prompts = 'marca_setor/Prompts_Resumo_Noticias_DBSCAN.xlsx'

# DeepSeek config
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
#DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
}

# Carregamento inicial
df = pd.read_excel(arq_api)
df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')
marcas = df['Canais'].dropna().unique()

# Fun√ß√£o para avaliar se a marca √© relevante na not√≠cia
def avaliar_relevancia_marca(marca, texto):
    prompt = (
        f"Avalie a relev√¢ncia da marca \"{marca}\" na seguinte not√≠cia.\n\n"
        "A marca deve ser considerada relevante quando influencia ou √© influenciada pelos fatos descritos no texto, mesmo que de forma indireta ou moderada. "
        "Caso contr√°rio, se a marca for apenas citada superficialmente, sem v√≠nculo com os eventos principais, considere irrelevante.\n\n"
        "Responda apenas com 'True' (se for relevante) ou 'False' (se n√£o for relevante).\n\n"
        f"Texto:\n{texto}"
    )

    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0,
        "max_tokens": 10
    }

    try:
        print(f"Enviando requisi√ß√£o ao DeepSeek para avaliar a marca '{marca}'...")
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        resposta = response.json()["choices"][0]["message"]["content"].strip()
        print(f"Resposta do DeepSeek: {resposta}")
        return resposta.lower().startswith("true")
    except Exception as e:
        print(f"Erro ao avaliar relev√¢ncia: {e}")
        return True  # Em caso de erro, assume como relevante para n√£o perder

# Avalia relev√¢ncia se ainda n√£o tiver sido feito
if 'RelevanciaMarca' not in df.columns or df['RelevanciaMarca'].isnull().all():
    print("Avaliando relev√¢ncia da marca nas not√≠cias...")
    df['RelevanciaMarca'] = df.apply(lambda row: avaliar_relevancia_marca(row['Canais'], row['TextoCompleto']), axis=1)

    # --- In√≠cio do ajuste para remover duplicatas relevantes ---
    # Define a ordem de prioridade das marcas
    # Removed duplicate 'Eldorado' from the list
    marca_order = ['JBS', 'J&F', 'PicPay', 'Eldorado', 'Joesley Batista', 'Wesley Batista', 'Banco Original']

    # Cria uma coluna tempor√°ria para ordena√ß√£o personalizada
    df['Marca_Order'] = pd.Categorical(df['Canais'], categories=marca_order, ordered=True)

    # Filtra apenas as not√≠cias relevantes (RelevanciaMarca == True) para aplicar a l√≥gica de desduplica√ß√£o
    df_relevantes = df[df['RelevanciaMarca'] == True].copy()

    # Ordena por Id e pela ordem de prioridade das marcas
    # Para IDs duplicados, a linha com a marca de maior prioridade (menor valor na categoria) vir√° primeiro
    df_relevantes_sorted = df_relevantes.sort_values(by=['Id', 'Marca_Order'], ascending=[True, True])

    # Remove duplicatas de Id, mantendo a primeira ocorr√™ncia (que ser√° a de maior prioridade devido √† ordena√ß√£o)
    df_relevantes_deduplicadas = df_relevantes_sorted.drop_duplicates(subset='Id', keep='first')

    # Remove a coluna tempor√°ria de ordena√ß√£o
    df_relevantes_deduplicadas = df_relevantes_deduplicadas.drop(columns=['Marca_Order'])

    # Separa as not√≠cias irrelevantes do DataFrame original
    df_irrelevantes = df[df['RelevanciaMarca'] == False].copy()

    # Remove a coluna tempor√°ria de ordena√ß√£o das irrelevantes tamb√©m
    df_irrelevantes = df_irrelevantes.drop(columns=['Marca_Order'])


    # Concatena as not√≠cias relevantes deduplicadas com as irrelevantes
    # A ordem das linhas n√£o √© garantida ap√≥s a concatena√ß√£o, mas as linhas corretas foram mantidas.
    # Se a ordem original for importante, pode ser necess√°rio um passo adicional para reordenar.
    df_final_relevancia = pd.concat([df_relevantes_deduplicadas, df_irrelevantes], ignore_index=True)

    # --- NOVO PASSO: Remover not√≠cias irrelevantes de df_final_relevancia ---
    print("Removendo not√≠cias irrelevantes de df_final_relevancia...")
    df_final_relevancia = df_final_relevancia[df_final_relevancia['RelevanciaMarca'] == True].copy()
    print(f"df_final_relevancia agora cont√©m {len(df_final_relevancia)} not√≠cias relevantes.")
    # --- FIM NOVO PASSO ---


    # Salva o DataFrame final (agora apenas relevantes) no arquivo de relev√¢ncia
    df_final_relevancia.to_excel("api/Favoritos_Marcas_Relevancia.xlsx", index=False)
    print("Arquivo api/Favoritos_Marcas_Relevancia.xlsx salvo (cont√©m apenas not√≠cias relevantes).")

    # --- Novo: Remover registros de final_df_small_marca que n√£o est√£o em df_final_relevancia (Id e Canais) ---
    # Carregar final_df_small_marca para processamento
    final_df_small_marca = pd.read_excel(arq_api)

    # Criar um conjunto de tuplas (Id, Canais) das not√≠cias que devem ser MANTIDAS (usando o df_final_relevancia j√° filtrado)
    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Filtrar final_df_small_marca para manter apenas as linhas cujos (Id, Canais) est√£o no conjunto
    # Ensure 'Canais' is treated as string in both DFs for consistent comparison
    final_df_small_marca['Canais'] = final_df_small_marca['Canais'].astype(str)
    df_final_relevancia['Canais'] = df_final_relevancia['Canais'].astype(str)

    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Apply the filter
    final_df_small_marca_processed = final_df_small_marca[
        final_df_small_marca.apply(lambda row: (row['Id'], row['Canais']) in ids_canais_to_keep, axis=1)
    ].copy()


    # Sobrescrever o arquivo Favoritos_Marcas_small.xlsx
    final_df_small_marca_processed.to_excel(arq_api, index=False)
    print(f"Arquivo {arq_api} sobrescrito ap√≥s remo√ß√£o de registros que n√£o foram considerados relevantes/priorit√°rios.")

    # --- Fim do novo ajuste ---


else:
    print("Coluna RelevanciaMarca j√° presente.")
    # Se a coluna j√° existe, mas o usu√°rio quer aplicar a desduplica√ß√£o,
    # voc√™ pode adicionar a l√≥gica de desduplica√ß√£o aqui tamb√©m,
    # lendo o arquivo existente e re-salvando ap√≥s a desduplica√ß√£o.
    # Por simplicidade nesta resposta, apenas imprimimos a mensagem.
    # Para um comportamento mais robusto, a l√≥gica de desduplica√ß√£o
    # poderia ser aplicada sempre, ou ter uma flag para control√°-la.
    # Como a coluna RelevanciaMarca j√° existe, vamos carregar o df
    # e aplicar a l√≥gica de desduplica√ß√£o para garantir que o arquivo
    # de relev√¢ncia esteja correto para as pr√≥ximas etapas.

    print("Aplicando desduplica√ß√£o em not√≠cias relevantes do arquivo existente...")
    # Removed duplicate 'Eldorado' from the list
    marca_order = ['JBS', 'J&F', 'PicPay', 'Eldorado', 'Joesley Batista', 'Wesley Batista', 'Banco Original']
    df['Marca_Order'] = pd.Categorical(df['Canais'], categories=marca_order, ordered=True)
    df_relevantes = df[df['RelevanciaMarca'] == True].copy()
    df_relevantes_sorted = df_relevantes.sort_values(by=['Id', 'Marca_Order'], ascending=[True, True])
    df_relevantes_deduplicadas = df_relevantes_sorted.drop_duplicates(subset='Id', keep='first')
    df_relevantes_deduplicadas = df_relevantes_deduplicadas.drop(columns=['Marca_Order'])

    df_irrelevantes = df[df['RelevanciaMarca'] == False].copy()
    df_irrelevantes = df_irrelevantes.drop(columns=['Marca_Order']) # Remove se existia antes ou foi criada

    df_final_relevancia = pd.concat([df_relevantes_deduplicadas, df_irrelevantes], ignore_index=True)

    # --- NOVO PASSO: Remover not√≠cias irrelevantes de df_final_relevancia (no bloco else) ---
    print("Removendo not√≠cias irrelevantes de df_final_relevancia (no bloco else)...")
    df_final_relevancia = df_final_relevancia[df_final_relevancia['RelevanciaMarca'] == True].copy()
    print(f"df_final_relevancia agora cont√©m {len(df_final_relevancia)} not√≠cias relevantes (no bloco else).")
    # --- FIM NOVO PASSO ---

    df_final_relevancia.to_excel("api/Favoritos_Marcas_Relevancia.xlsx", index=False)
    print("Arquivo api/Favoritos_Marcas_Relevancia.xlsx re-salvo ap√≥s desduplica√ß√£o (cont√©m apenas not√≠cias relevantes).")

    # --- Novo: Remover registros de final_df_small_marca que n√£o est√£o em df_final_relevancia (Id e Canais) (caso a coluna j√° existisse) ---
    # Carregar final_df_small_marca para processamento
    final_df_small_marca = pd.read_excel(arq_api)

    # Criar um conjunto de tuplas (Id, Canais) das not√≠cias que devem ser MANTIDAS (usando o df_final_relevancia j√° filtrado)
    # Ensure 'Canais' is treated as string in both DFs for consistent comparison
    final_df_small_marca['Canais'] = final_df_small_marca['Canais'].astype(str)
    df_final_relevancia['Canais'] = df_final_relevancia['Canais'].astype(str)

    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Apply the filter
    final_df_small_marca_processed = final_df_small_marca[
        final_df_small_marca.apply(lambda row: (row['Id'], row['Canais']) in ids_canais_to_keep, axis=1)
    ].copy()

    # Sobrescrever o arquivo Favoritos_Marcas_small.xlsx
    final_df_small_marca_processed.to_excel(arq_api, index=False)
    print(f"Arquivo {arq_api} sobrescrito ap√≥s remo√ß√£o de registros que n√£o foram considerados relevantes/priorit√°rios (coluna RelevanciaMarca j√° existia).")
    # --- Fim do novo ajuste (caso a coluna j√° existisse) ---


# --- Fim do ajuste para remover duplicatas relevantes ---

"""##Refinamento: Verificar se os **RESUMOS** tratam do **MESMO ASSUNTO** - **VERSAO 2**"""

# Etapa 2: Resumo de at√© 40 palavras, agrupamento sem√¢ntico e gera√ß√£o de resumos finais com refinamento por subtemas

import pandas as pd
import os
import requests
import re

from dotenv import load_dotenv

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY n√£o foi encontrada no arquivo .env")

#if "DEEPSEEK_API_KEY" not in os.environ:
#    try:
#        from google.colab import userdata
#        os.environ["DEEPSEEK_API_KEY"] = userdata.get("DEEPSEEK_API_KEY")
#    except Exception as e:
#        raise RuntimeError("Erro ao acessar a chave DEEPSEEK_API_KEY nos Secrets do Colab.") from e

arq_textos = "dados/api/Favoritos_Marcas_small.xlsx"
arq_saida_final = "dados/marca_setor/Resumos_Finais_DeepSeek.xlsx"

DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
#DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
}

def gerar_resumo_40(texto, id_):
    print(f"üìù Gerando resumo curto para not√≠cia ID: {id_}...")
    prompt = "Resuma o conte√∫do a seguir em at√© 40 palavras.\n\n" + texto
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 100
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"Erro ao gerar resumo curto para ID {id_}: {e}")
        return ""

def agrupar_por_similaridade(resumos):
    print(f"üîó Enviando {len(resumos)} resumos para agrupamento sem√¢ntico via DeepSeek...")
    prompt = (
        "Agrupe os resumos abaixo por similaridade de assunto. "
        "Considere como similar n√£o apenas assuntos id√™nticos, mas tamb√©m aqueles com forte rela√ß√£o tem√°tica, como diferentes aspectos de um mesmo setor, empresa ou impacto.\n"
        "Retorne uma √∫nica linha com os n√∫meros dos grupos separados por v√≠rgula, na mesma ordem dos resumos.\n"
        "Exemplo: 1,1,2,2,3\n\n"
    )
    for i, resumo in enumerate(resumos):
        prompt += f"Resumo {i+1}: {resumo}\n"
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 400
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"].strip()
        print("üì§ Resposta bruta do agrupamento:")
        print(content)
        linha_grupos = next((l for l in content.splitlines() if re.match(r"^\d+(,\d+)*$", l.strip())), "")
        if not linha_grupos:
            print("‚ö†Ô∏è Nenhuma linha de grupo reconhecida. Conte√∫do retornado:")
            for linha in content.splitlines():
                print(f"> {linha}")
        else:
            print(f"‚úÖ Linha de grupos detectada: {linha_grupos}")
        grupos = [int(g) for g in linha_grupos.strip().split(",")] if linha_grupos else list(range(len(resumos)))
        if len(grupos) != len(resumos):
            print("‚ö†Ô∏è N√∫mero de grupos n√£o bate. Atribuindo grupos √∫nicos...")
            return list(range(len(resumos)))
        return grupos
    except Exception as e:
        print(f"Erro ao agrupar resumos: {e}")
        return list(range(len(resumos)))

def gerar_resumo_160(textos, marca):
    corpo = "\n--- NOT√çCIA ---\n".join(textos)
    prompt = f"Gere um resumo √∫nico de at√© 160 palavras para as not√≠cias a seguir sobre a marca '{marca}', destacando os fatos mais importantes:\n\n{corpo}"
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 400
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        texto = response.json()["choices"][0]["message"]["content"].strip()
        texto = re.sub(r"^\*\*?Resumo.*?\*\*?\s*", "", texto, flags=re.IGNORECASE)
        texto = re.sub(r"\*\(160 palavras\)\*|\*\(Exatamente 160 palavras\)\*", "", texto, flags=re.IGNORECASE)
        texto = re.sub(r"\n{2,}", "\n", texto).strip()
        return texto
    except Exception as e:
        print(f"Erro ao gerar resumo final: {e}")
        return ""

def reavaliar_grupo(grupo_df):
    resumos = grupo_df['Resumo40'].tolist()
    ids = grupo_df['Id'].tolist()
    subgrupos = agrupar_por_similaridade(resumos)
    grupo_df = grupo_df.copy()
    grupo_df['SubGrupoID'] = subgrupos
    return grupo_df

try:
    df = pd.read_excel(arq_textos)
    df['Id'] = df['Id'].astype(str)
    df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')

    lista_resumos = []
    for _, row in df.iterrows():
        resumo = gerar_resumo_40(row['TextoCompleto'], row['Id'])
        lista_resumos.append({"Id": row['Id'], "Marca": row['Canais'], "TextoCompleto": row['TextoCompleto'], "Resumo40": resumo})

    df_resumos = pd.DataFrame(lista_resumos)
    grupos_iniciais = agrupar_por_similaridade(df_resumos['Resumo40'].tolist())
    df_resumos['GrupoID'] = grupos_iniciais

    refinado = []
    for (marca, grupo), grupo_df in df_resumos.groupby(['Marca', 'GrupoID']):
        if len(grupo_df) > 1:
            print(f"üîç Reavaliando grupo {grupo} da marca {marca}...")
            grupo_df = reavaliar_grupo(grupo_df)
        else:
            grupo_df['SubGrupoID'] = 0
        refinado.append(grupo_df)

    df_refinado = pd.concat(refinado).reset_index(drop=True)

    resultados = []
    for (marca, grupo, subgrupo), df_sub in df_refinado.groupby(['Marca', 'GrupoID', 'SubGrupoID']):
        textos = df_sub['TextoCompleto'].tolist()
        ids = df_sub['Id'].tolist()
        resumo_final = gerar_resumo_160(textos, marca)
        resultados.append({
            "Marca": marca,
            "GrupoID": f"{marca}_G{grupo}_S{subgrupo}",
            "QtdNoticias": len(ids),
            "Ids": ','.join(ids),
            "Resumo": resumo_final
        })

    df_final = pd.DataFrame(resultados)
    df_final.to_excel(arq_saida_final, index=False)
    print(f"‚úÖ Resumos finais salvos em {arq_saida_final}")

except Exception as e:
    print(f"Erro geral no processamento: {e}")



"""#Chamada da API - **SETOR**"""


# Chamada da API para trazer os favoritos da MVC Admin Impressos TOP 4 J&F Hoje
import requests
import pandas as pd
import os
import json
import re

# Caminho para o arquivo externo com as configura√ß√µes da API
config_file = os.path.join(JF_path, "dados", "api_setor_configs.json")  # Nome do arquivo de configura√ß√£o

# Carrega as configura√ß√µes da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configura√ß√µes da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se n√£o, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisi√ß√£o POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisi√ß√£o foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame √† lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisi√ß√£o para {url}: {response.status_code}")

# Concatena todos os DataFrames em um √∫nico DataFrame
final_df_setor = pd.concat(all_dfs, ignore_index=True)

# --- Added code to remove duplicates based on IdVeiculo + Titulo + DataVeiculacao ---
# Remover duplicatas de IdVeiculo + Titulo
# Convert 'DataVeiculacao' to datetime objects, coercing errors
final_df_setor['DataVeiculacao'] = pd.to_datetime(final_df_setor['DataVeiculacao'], errors='coerce')

# Sort by 'IdVeiculo', 'Titulo', and 'DataVeiculacao' in descending order
final_df_setor = final_df_setor.sort_values(by=['IdVeiculo', 'Titulo', 'DataVeiculacao'], ascending=[True, True, False])

# Remove duplicates based on 'IdVeiculo' and 'Titulo', keeping the first occurrence (which is the latest due to sorting)
final_df_setor = final_df_setor.drop_duplicates(subset=['IdVeiculo', 'Titulo'], keep='first').reset_index(drop=True)
# --- End of added code ---


# Vari√°vel com os t√≠tulos a ignorar
titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "hor√≥scopo", \
                     "mensagens", "MIRIAM LEIT√ÉO", "M√îNICA BERGAMO", "multitela", "Obitu√°rio", \
                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
                     "coluna do estad√£o", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_setor['Titulo'] = final_df_setor['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo come√ßa com os termos a ignorar (compara√ß√£o em min√∫sculas)
# Criar uma m√°scara booleana para as linhas a serem mantidas
mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a m√°scara para remover as linhas indesejadas
final_df_setor = final_df_setor[mask].copy()

# --- Added code to remove illegal characters ---
# Function to remove illegal characters
def remove_illegal_chars(text):
    if isinstance(text, str):
        # Remove characters not allowed in XML (and thus often in Excel)
        # This regex removes characters in the range U+0000 to U+0008, U+000B, U+000C, U+000E to U+001F
        illegal_chars = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')
        return illegal_chars.sub('', text)
    return text

# Apply the function to the 'Conteudo' column
final_df_setor['Conteudo'] = final_df_setor['Conteudo'].apply(remove_illegal_chars)
# --- End of added code ---

final_df_setor.to_excel(arq_api_original_setor_inter, index=False)

print("Arquivo Excel salvo como ", arq_api_original_setor)

# ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì IN√çCIO DO TRECHO PARA DESPREZAR REGISTROS DO ARQUIVO DE SETOR ‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì‚Üì

# Assuming final_df_setor and w_marcas are already defined in your environment

# 1. Eliminar registros com Secao indesejada (em min√∫sculas)
secoes_a_ignorar = ["esportes", "cotidiano", "folha corrida", "rio", "sa√∫de", "opini√£o", "na web", \
                    "classificados", "cultura", "ilustrada"]

# Create a list of the lowercased and stripped versions of the sections to ignore
secoes_a_ignorar_cleaned = [s.strip().lower() for s in secoes_a_ignorar]

# Clean the 'Secao' column by stripping whitespace and converting to lowercase
final_df_setor['Secao_cleaned'] = final_df_setor['Secao'].astype(str).str.strip().str.lower()

# Now apply the filter using the cleaned column and the cleaned list of sections to ignore
final_df_setor_filtered = final_df_setor[
    ~final_df_setor['Secao_cleaned'].isin(secoes_a_ignorar_cleaned)
].copy()

# You can drop the temporary 'Secao_cleaned' column if you don't need it later
final_df_setor_filtered = final_df_setor_filtered.drop(columns=['Secao_cleaned'])

# 2. Eliminar registros cujo campo CanaisCommodities atenda √†s novas condi√ß√µes
# Condi√ß√£o para CanaisCommodities conter "Obitu√°rios"
condition_contains_obituarios = final_df_setor_filtered['CanaisCommodities'].astype(str).str.lower().str.contains('obitu√°rios', na=False)

# Combinar as condi√ß√µes de exclus√£o: (cont√©m Obitu√°rios)
mask_exclude_canais = condition_contains_obituarios

# Filtrar o DataFrame para manter as linhas que N√ÉO atendem √†s condi√ß√µes de exclus√£o
final_df_setor_filtered = final_df_setor_filtered[~mask_exclude_canais].copy()

# 3. Eliminar registros cujo campo Conteudo contiver qualquer um dos termos constantes da vari√°vel marcas_a_ignorar (comparar em min√∫sculas)
# Criar um padr√£o regex para buscar qualquer uma das palavras em marcas_a_ignorar, com boundary words
marcas_a_ignorar_lower = [marca.lower() for marca in marcas_a_ignorar]
pattern_marcas_a_ignorar = r'\b(' + '|'.join(re.escape(marca) for marca in marcas_a_ignorar_lower) + r')\b'

# --- Modified this line to use the already cleaned 'Conteudo' column ---
final_df_setor_filtered = final_df_setor_filtered[
    ~final_df_setor_filtered['Conteudo'].str.lower().str.contains(pattern_marcas_a_ignorar, na=False)
].copy()
# --- End of modification ---

# 4. Eliminar registros onde no campo Conteudo tenha, numa mesma linha,
# que comece com "leia mais" ou "leia tamb√©m", e em outro lugar qualquer
# da mesma linha, contiver qualquer um dos termos constantes da vari√°vel marcas_a_ignorar (comparar em min√∫sculas)

# Primeiro, lidar com valores NaN na coluna 'Conteudo'
final_df_setor_filtered['Conteudo'] = final_df_setor_filtered['Conteudo'].fillna('')

# Fun√ß√£o para verificar a condi√ß√£o combinada em cada linha
def check_leia_mais_and_marcas_a_ignorar(content, marcas_a_ignorar):
    content_lower = content.lower()
    starts_with_leia = content_lower.strip().startswith("leia mais") or content_lower.strip().startswith("leia tamb√©m")

    # Check if any of the marcas_a_ignorar are present in the lowercased content
    marca_present = any(re.search(r'\b' + re.escape(marca.lower()) + r'\b', content_lower) for marca in marcas_a_ignorar)

    return starts_with_leia and marca_present

# Aplicar a fun√ß√£o para criar uma m√°scara booleana
# --- Modified this line to use the already cleaned 'Conteudo' column ---
mask_leia_mais_and_marcas_a_ignorar = final_df_setor_filtered['Conteudo'].apply(
    lambda x: check_leia_mais_and_marcas_a_ignorar(x, marcas_a_ignorar)
)
# --- End of modification ---

# Filtrar o DataFrame para remover as linhas que correspondem √† condi√ß√£o
final_df_setor_filtered = final_df_setor_filtered[~mask_leia_mais_and_marcas_a_ignorar].copy()

# O DataFrame 'final_df_setor_filtered' agora cont√©m os dados ap√≥s todas as filtragens.


print(f"N√∫mero de registros antes da filtragem: {len(final_df_setor)}")
print(f"N√∫mero de registros ap√≥s a filtragem: {len(final_df_setor_filtered)}")

# Voc√™ pode renome√°-lo para final_df_setor se quiser substituir o original.
final_df_setor = final_df_setor_filtered.copy()

# ‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë FINAL DO TRECHO PARA DESPREZAR REGISTROS DO ARQUIVO DE SETOR ‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë‚Üë


# Grava o DataFrame final em um arquivo Excel

final_df_setor.to_excel(arq_api_original_setor, index=False)

# Criar o DataFrame 'final_df_small' ap√≥s a filtragem
final_df_setor_small = final_df_setor[['Id', 'Titulo', 'Conteudo', 'IdVeiculo']].copy()


final_df_setor_small.to_excel(arq_api_setor, index=False)

print("Arquivo Excel salvo como ", arq_api_original_setor)
print("Arquivo Excel salvo como ", arq_api_setor)

"""#Gerar Grupos e Prompts de Resumo - **SETOR**"""

#import pandas as pd
from sentence_transformers import SentenceTransformer, util
from sklearn.cluster import DBSCAN
import numpy as np
import re # Importar re para limpeza do tema
from collections import Counter # Importar Counter para contagem de termos

# 1. Carrega os dados (todas as not√≠cias)
df = pd.read_excel(arq_api_setor)
df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')

# 2. Carrega o modelo de embeddings (ainda n√£o usaremos para filtragem, mas pode ser √∫til para outras an√°lises)
# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# 3. Identificar not√≠cias relacionadas aos temas (usando frequ√™ncia de termos) e calcular a pontua√ß√£o de relev√¢ncia
temas_termos = {
    "Setor de Papel e Celulose": ["papel", "celulose", "fibra", "eucalipto", "pulp", "paper", "cellulose"],
    "Setor de Minera√ß√£o": ["minera√ß√£o", "mineradora", "min√©rio", "ferro", "n√≠quel", "ouro", "metal", "geologia", "jazida", "mina", "mining"],
    "Setor de Agroneg√≥cios": ["agroneg√≥cio", "agro", "pecu√°ria", "lavoura", "safra", "colheita", "gr√£os", "soja", "milho", "carne", "laranjas", \
                              "exporta√ß√£o agr√≠cola", "rural", "agribusiness", "gripe avi√°ria", "av√≠colas", "derivados", "frango", "ovos", \
                              "cacau", "cr√©dito rural", "h5n1", "rastreabilidade", "tecnologia agr√≠cola", "inova√ß√£o agr√≠cola", "caprinos", \
                              "ovinos", "abpa"],
    "Setor de Educa√ß√£o": ["educa√ß√£o", "escola", "universidade", "ensino", "aluno", "professor", "faculdade", "curso", "vestibular", \
                          "enem", "educacional", "estudante", "estudantes", "vestibulares", "educacional", "educacionais", "docente", \
                          "aprendizagem", "ead", "mec"],
    "Setor de Energia": ["energia", "el√©trica", "usina", "hidrel√©trica", "termel√©trica", "e√≥lica", "solar", "transmiss√£o", "distribui√ß√£o", \
                         "gasolina", "diesel", "etanol", "combust√≠vel", "petr√≥leo", "g√°s", "conta de luz", "mme"],
    "Setor de Finan√ßas": ["finan√ßas", "banco", "cr√©dito", "investimento", "investimentos", "mercado financeiro", "mercados", "a√ß√£o", "renda fixa", \
                          "c√¢mbio", "d√≠vida", "lucro", "capital", "IPO", "banco central", "pol√≠tica monet√°ria", "pol√≠tica econ√¥mica", "governo", \
                          "a√ß√µes", "fundos", "balan√ßo", "balan√ßos", "bolsa", "nasdaq", "etf", "tributa√ß√£o", "contribuinte", "selic", "juros", \
                          "precat√≥rios", "infla√ß√£o", "deficit fiscal", "ibs", "cbs", "ibovespa", "b3"],
    "Setor de √ìleo de G√°s": ["√≥leo", "g√°s", "petr√≥leo", "explora√ß√£o", "refinaria", "gasoduto", "po√ßo", "onshore", "offshore", \
                             "glp", "petrobras", "anp"],
    "Justi√ßa": ["justi√ßa", "judici√°rio", "tribunal", "juiz", "minist√©rio p√∫blico", "processo", "senten√ßa", "condena√ß√£o", "advogado", \
                "lei", "legal", "stf", "supremo", "pena", "penas", "jurisprud√™ncia", "pgr", "julgamento", "recurso", "judicial", \
                "den√∫ncia", "acusa√ß√£o", "stj", "cnj", "golpe de estado", "penduricalhos", "agu", "alexandre de moraes"],
    "Meio Ambiente e ESG": ["meio ambiente", "sustentabilidade", "ambiental", "ambientalistas", "ecologia", "desmatamento", \
                            "polui√ß√£o", "clima", "ESG", "governan√ßa ambiental", "responsabilidade social", "emiss√£o de carbono", \
                            "biodiversidade", "amaz√¥nia", "floresta", "explora√ß√£o", "cerrado", "mata atl√¢ntica", "cop30", \
                            "licenciamento ambiental", "cr√©ditos de carbono", "ibama"],
    "Pol√≠tica - Governo e Congresso Nacional": ["pol√≠tica", "governo", "congresso", "elei√ß√£o", "elei√ß√µes", "reelei√ß√£o", "partido", "partidos", \
                                                "ministro", "ministra", "presidente", "ex-presidente", "senado", "c√¢mara", "deputado", "deputada", \
                                                "senador", "senadora", "urnas", "executivo", "legislativo", "tse", "planalto", "primeira-dama", \
                                                "casa civil", "inss", "fraude", "cpmi", "trama golpista"],
    "Setor de Esportes": ["esporte", "futebol", "basquete", "v√¥lei", "atletismo", "olimp√≠adas", "copa", "campeonato", "clube", "jogador", \
                          "treinador", "partida", "competi√ß√£o", "cbf", "federa√ß√£o", "federa√ß√µes", "clubes", "atleta", "atletas", \
                          "arbitragem", "fifa", "xaud", "ednaldo"] # Novo setor adicionado
}

# Definir os IDs dos ve√≠culos priorit√°rios
veiculos_prioritarios = [10459, 675]
pontuacao_extra_veiculo = 100 # Pontua√ß√£o extra para not√≠cias desses ve√≠culos (ajuste este valor se necess√°rio)

def calculate_relevance_score(text, id_veiculo, temas_termos, veiculos_prioritarios, pontuacao_extra_veiculo):
    """
    Calcula uma pontua√ß√£o de relev√¢ncia para a not√≠cia.
    A pontua√ß√£o √© baseada na frequ√™ncia dos termos chave dos temas, no tamanho do texto
    e em uma pontua√ß√£o extra para ve√≠culos priorit√°rios.
    Retorna a pontua√ß√£o de relev√¢ncia e o tema preponderante.
    """
    text_lower = text.lower()
    theme_counts = Counter()
    total_term_count = 0

    for tema, termos in temas_termos.items():
        for termo in termos:
            count = len(re.findall(r'\b' + re.escape(termo) + r'\b', text_lower))
            theme_counts[tema] += count
            total_term_count += count

    # Remover temas com contagem zero para identificar o tema preponderante
    theme_counts_filtered = {tema: count for tema, count in theme_counts.items() if count > 0}

    preponderant_theme = None
    if theme_counts_filtered:
        preponderant_theme = max(theme_counts_filtered, key=theme_counts_filtered.get)

    # Calcular o tamanho do texto (n√∫mero de palavras)
    text_size = len(text.split())

    # Calcular a pontua√ß√£o do ve√≠culo
    pontuacao_veiculo = pontuacao_extra_veiculo if id_veiculo in veiculos_prioritarios else 0

    # Calcular a pontua√ß√£o total de relev√¢ncia
    # Podemos simplesmente somar a contagem total de termos, o tamanho do texto e a pontua√ß√£o do ve√≠culo.
    # A rela√ß√£o exata entre esses fatores pode ser ajustada se necess√°rio.

    # Removi temporariamente o text_size e pontuacao_veiculo
    #relevance_score = total_term_count + text_size + pontuacao_veiculo

    relevance_score = total_term_count

    return relevance_score, preponderant_theme

# Aplicar a fun√ß√£o para calcular a pontua√ß√£o e identificar o tema preponderante
results = df.apply(lambda row: calculate_relevance_score(row['TextoCompleto'], row['IdVeiculo'], temas_termos, veiculos_prioritarios, pontuacao_extra_veiculo), axis=1)

# Separar os resultados em novas colunas
df['RelevanceScore'], df['TemaPreponderante'] = zip(*results)

# **Adicionar este passo para remover not√≠cias do 'Setor de Esportes'**
#print("N√∫mero de not√≠cias antes da filtragem: ", len(df))
#df_relevante = df[df['TemaPreponderante'] != 'Setor de Esportes'].copy()
#print("N√∫mero de not√≠cias ap√≥s a filtragem: ", len(df_relevante))

# Filtrar not√≠cias que se enquadram em algum tema preponderante (ainda necess√°rio)
#df_relevante = df[df['TemaPreponderante'].notna()].copy()

# Identificar not√≠cias do 'Setor de Esportes'
df_esportes = df[df['TemaPreponderante'] == 'Setor de Esportes'].copy()

# Filtrar o DataFrame para remover not√≠cias do 'Setor de Esportes' E not√≠cias sem tema
df_relevante = df[
    (df['TemaPreponderante'] != 'Setor de Esportes') &  # Remove Setor de Esportes
    (df['TemaPreponderante'].notna())                 # Remove not√≠cias sem tema
].copy()

print("N√∫mero de not√≠cias antes da filtragem: ", len(df))
print(f"N√∫mero de not√≠cias do Setor de Esportes removidas: {len(df_esportes)}")
print("N√∫mero de not√≠cias ap√≥s a filtragem: ", len(df_relevante))

# Definir os temas priorit√°rios e a quantidade de not√≠cias a serem selecionadas de cada um
temas_prioritarios = {
    "Pol√≠tica - Governo e Congresso Nacional": 8,
    "Setor de Finan√ßas": 7,
    "Justi√ßa": 5,
    "Setor de Agroneg√≥cios": 5
}

df_top_noticias_list = []

# Selecionar as top not√≠cias dos temas priorit√°rios
for tema, qtd in temas_prioritarios.items():
    df_tema = df_relevante[df_relevante['TemaPreponderante'] == tema].sort_values(by='RelevanceScore', ascending=False)
    df_top_noticias_list.append(df_tema.head(qtd))

# Filtrar os temas restantes
temas_restantes = [tema for tema in df_relevante['TemaPreponderante'].unique() if tema not in temas_prioritarios]
df_restantes = df_relevante[df_relevante['TemaPreponderante'].isin(temas_restantes)].copy()

# Selecionar as top 7 not√≠cias dos temas restantes
if not df_restantes.empty:
    df_restantes_sorted = df_restantes.sort_values(by='RelevanceScore', ascending=False)
    df_top_noticias_list.append(df_restantes_sorted.head(7))

# Concatenar todos os DataFrames das not√≠cias selecionadas
df_top_noticias = pd.concat(df_top_noticias_list, ignore_index=True)

# 6. Criar prompts para cada not√≠cia selecionada
prompts = []
for index, row in df_top_noticias.iterrows():
    texto_noticia = row['TextoCompleto']
    tema_preponderante = row['TemaPreponderante']
    ids_noticia = str(row['Id']) # ID da not√≠cia
    relevance_score = row['RelevanceScore'] # Obter a pontua√ß√£o de relev√¢ncia
    id_veiculo_noticia = row['IdVeiculo'] # Obter o IdVeiculo da not√≠cia

    # Criar o prompt
    corpo = texto_noticia
    prompt = (
        f"Resuma a not√≠cia abaixo em no m√°ximo 90 palavras. "
        f"O resumo deve focar nos aspectos relacionados ao tema de {tema_preponderante}:\n\n"
        f"{corpo}"
    )

    prompts.append({
        "Ids": ids_noticia,
        "Tipo": "Not√≠cia Individual",
        "Prompt": prompt,
        "Tema": tema_preponderante, # Adicionar coluna de Tema Preponderante
        "RelevanceScore": relevance_score, # Adicionar a pontua√ß√£o de relev√¢ncia
        "IdVeiculo": id_veiculo_noticia # Adicionar o IdVeiculo
    })

# 7. Salva em Excel
df_prompts = pd.DataFrame(prompts)

# Ordenar por tema para melhor organiza√ß√£o (ou pela ordem que preferir para o arquivo de sa√≠da)
df_prompts = df_prompts.sort_values(by='Tema')

df_prompts.to_excel(arq_prompts_setor, index=False)
print("Arquivo salvo: ", arq_prompts_setor)

"""#Processar pelo DeepSeek - **SETOR**"""

##!pip install requests

#import pandas as pd
import requests
import time

# 1. Configura√ß√£o

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY n√£o foi encontrada no arquivo .env")

API_URL = "https://api.deepseek.com/v1/chat/completions"

HEADERS = {
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
    "Content-Type": "application/json"
}

# 2. Carrega prompts gerados
df = pd.read_excel(arq_prompts_setor)

# 3. Fun√ß√£o para chamar a API
def resumir_prompt(prompt_text):
    payload = {
#        "model": "deepseek/deepseek-chat",
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "Voc√™ √© um jornalista profissional especializado em resumir not√≠cias."},
            {"role": "user", "content": prompt_text}
        ],
        "temperature": 0.7
    }

    try:
        #print(payload)
        response = requests.post(API_URL, headers=HEADERS, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"Erro: {e}"

# 4. Processa todos os prompts
resumos = []
for idx, row in df.iterrows():
    print(f"Processando grupo {idx+1}/{len(df)} do tema {row['Tema']}...")

    prompt = row['Prompt']
    resumo = resumir_prompt(prompt)

    resumos.append({
        "Tema": row['Tema'],
        #"GrupoID": row['GrupoID'],
        #"QtdNoticias": row['QtdNoticias'],
        "Id": row['Ids'],
        "Resumo": resumo
    })

    time.sleep(2)  # pausa para respeitar limites da API

# 5. Salva resultados
df_resumo_setor = pd.DataFrame(resumos)
df_resumo_setor.to_excel(arq_results_setor, index=False)
print("Arquivo salvo: ", arq_results_setor)

"""#Chamada da API - **EDITORIAIS**"""

# Chamada da API para trazer os EDITORIAIS da MVC CommoditiesTemp J&F Eitoriais TOP4

# Caminho para o arquivo externo com as configura√ß√µes da API
config_file = os.path.join(JF_path, "dados", "api_editorial_configs.json")  # Nome do arquivo de configura√ß√£o

# Carrega as configura√ß√µes da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configura√ß√µes da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se n√£o, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisi√ß√£o POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisi√ß√£o foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame √† lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisi√ß√£o para {url}: {response.status_code}")

# Concatena todos os DataFrames em um √∫nico DataFrame
final_df_editorial = pd.concat(all_dfs, ignore_index=True)

# Vari√°vel com os t√≠tulos a ignorar
#titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "hor√≥scopo", \
#                     "mensagens", "MIRIAM LEIT√ÉO", "M√îNICA BERGAMO", "multitela", "Obitu√°rio", \
#                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
#                     "coluna do estad√£o", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_editorial['Titulo'] = final_df_editorial['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo come√ßa com os termos a ignorar (compara√ß√£o em min√∫sculas)
# Criar uma m√°scara booleana para as linhas a serem mantidas
#mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a m√°scara para remover as linhas indesejadas
#final_df_setor = final_df_setor[mask].copy()

# Grava o DataFrame final em um arquivo Excel

final_df_editorial.to_excel(arq_api_original_editorial, index=False)

# Criar o DataFrame 'final_df_small' ap√≥s a filtragem
final_df_editorial_small = final_df_editorial[['Id', 'Titulo', 'Conteudo', 'IdVeiculo']].copy()


final_df_editorial_small.to_excel(arq_api_editorial, index=False)

print("Arquivo Excel salvo como ", arq_api_original_editorial)
print("Arquivo Excel salvo como ", arq_api_editorial)

"""#Chamada da API - **SPECIALS (Colunistas, Editoriais, 1a P√°gina)**"""

# Chamada da API para trazer os Colunistas, Editoriais e 1a P√°gina da MVC CommoditiesTemp J&F COL-EDT-CAPA TOP4

# Caminho para o arquivo externo com as configura√ß√µes da API
config_file = os.path.join(JF_path, "dados", "api_SPECIALS_configs.json")  # Nome do arquivo de configura√ß√£o

# Carrega as configura√ß√µes da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configura√ß√µes da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se n√£o, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisi√ß√£o POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisi√ß√£o foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame √† lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisi√ß√£o para {url}: {response.status_code}")

# Concatena todos os DataFrames em um √∫nico DataFrame
final_df_SPECIALS = pd.concat(all_dfs, ignore_index=True)

# Vari√°vel com os t√≠tulos a ignorar
#titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "hor√≥scopo", \
#                     "mensagens", "MIRIAM LEIT√ÉO", "M√îNICA BERGAMO", "multitela", "Obitu√°rio", \
#                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
#                     "coluna do estad√£o", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_SPECIALS['Titulo'] = final_df_SPECIALS['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo come√ßa com os termos a ignorar (compara√ß√£o em min√∫sculas)
# Criar uma m√°scara booleana para as linhas a serem mantidas
#mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a m√°scara para remover as linhas indesejadas
#final_df_setor = final_df_setor[mask].copy()

# Grava o DataFrame final em um arquivo Excel

final_df_SPECIALS.to_excel(arq_api_original_SPECIALS, index=False)


# Criar o DataFrame 'final_df_small' ap√≥s a filtragem
final_df_SPECIALS_small = final_df_SPECIALS[['Id', 'Canais']].copy()


final_df_SPECIALS_small.to_excel(arq_api_SPECIALS, index=False)

print("Arquivo Excel salvo como ", arq_api_original_SPECIALS)
print("Arquivo Excel salvo como ", arq_api_SPECIALS)

"""#Gera√ß√£o do Resumo final"""

"""##Vers√£o **Preliminar**"""


import pandas as pd
import pyshorteners
from docx import Document
from docx.shared import Pt  # Import Pt for point size
from docx.enum.text import WD_LINE_SPACING  # Import line spacing enum
from docx.shared import Inches
from docx.text.run import Run
from docx.enum.style import WD_STYLE_TYPE # Import WD_STYLE_TYPE
import re
import time # Importar time para pausa


# 1. Carregar os DataFrames
df_resumo_marca = pd.read_excel(arq_results_final) # Renomeado para clareza
final_df_marca = pd.read_excel(arq_api_original) # Renomeado para clareza
df_resumo_setor = pd.read_excel(arq_results_setor) # Carregar resultados do setor
final_df_setor = pd.read_excel(arq_api_original_setor) # Carregar dados originais do setor
final_df_editorial = pd.read_excel(arq_api_original_editorial) # Carregar dados originais dos editoriais
final_df_SPECIALS_small = pd.read_excel(arq_api_SPECIALS) # Carregar dados dos SPECIALS


# Load the final_df_small generated in the MARCA section
# We assume arq_api points to the small excel file from the MARCA section
try:
    final_df_small_marca = pd.read_excel(arq_api)
except FileNotFoundError:
    print(f"Erro: Arquivo {arq_api} n√£o encontrado. Certifique-se de que a se√ß√£o de MARCA foi executada.")
    # You might want to exit or handle this case differently
    final_df_small_marca = pd.DataFrame() # Create empty DataFrame to avoid further errors


# 2. Inicializar o documento DOCX
document = Document()

# Configurar o estilo Normal para remover espa√ßo ap√≥s o par√°grafo e usar espa√ßamento simples
styles = document.styles
style = styles['Normal']
font = style.font
font.name = 'Calibri'  # Set the font to Calibri
style.paragraph_format.space_after = Pt(0)
style.paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE


# --- Se√ß√£o Original para resumos de Marca ---
# 3. Iterar sobre df_resumo (Marca)
print(f"Processando {len(df_resumo_marca)} resumos de Marca...") # Debug print
for index, row_marca in df_resumo_marca.iterrows(): # Renomeado para clareza
    # Inicializar a string para cada grupo de not√≠cias
    group_string = ""

    # 4. Iterar sobre os IDs das not√≠cias (Marca)
    # Check if 'Ids' column exists and is not None
    if 'Ids' not in row_marca or pd.isna(row_marca['Ids']):
        print(f"Aviso: Linha {index} no df_resumo_marca n√£o tem IDs v√°lidos. Pulando.")
        continue

    for news_id_str in str(row_marca['Ids']).split(','): # Ensure it's a string
        try:
            news_id = int(news_id_str.strip())  # Convert to integer, strip whitespace
        except ValueError:
            print(f"Aviso: N√£o foi poss√≠vel converter ID '{news_id_str}' para inteiro na linha {index} de df_resumo_marca. Pulando.")
            continue # Skip this ID if not a valid number


        # 5. Consultar informa√ß√µes no final_df_marca
        # Use final_df_marca for original info (like Veiculo, UrlVisualizacao, CanaisCommodities)
        news_info_marca = final_df_marca[final_df_marca['Id'] == news_id]
        if news_info_marca.empty:
             print(f"Aviso: ID {news_id} n√£o encontrado em final_df_marca para resumo de Marca. Pulando.")
             continue # Skip this news if not found

        news_info_marca = news_info_marca.iloc[0]

        w_veiculo_marca = news_info_marca['Veiculo']
        w_url_marca = news_info_marca['UrlVisualizacao']
        #w_canais_commodities_marca = news_info_marca['CanaisCommodities'] # Obter CanaisCommodities (se dispon√≠vel)


        # 6. Encurtar a URL
        s = pyshorteners.Shortener()
        retries = 3  # Number of retries
        short_url_marca = w_url_marca # Inicializa com a URL original
        for i in range(retries):
            try:
                short_url_marca = s.tinyurl.short(w_url_marca)
                break  # Exit the loop if successful
            except requests.exceptions.RequestException as e:
                # Check if the error is due to a redirect or other network issue
                if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                     print(f"Erro de conex√£o/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Marca ID {news_id}: {e}")
                else:
                    print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Marca ID {news_id}: {e}")

                if i < retries - 1:
                    time.sleep(2)  # Wait before retrying
                else:
                    print(f"Falha ao encurtar URL para Marca ID {news_id} ap√≥s {retries} tentativas. Usando URL original.")
                    short_url_marca = w_url_marca # Use original URL if all retries fail


        # **Atualizar o DataFrame arq_api** (Atualiza final_df_small_marca)
        # Certifica-se de que a linha existe antes de tentar atualizar
        if news_id in final_df_small_marca['Id'].values:
             final_df_small_marca.loc[final_df_small_marca['Id'] == news_id, 'ShortURL'] = short_url_marca
        else:
             # This might happen if the small DF was filtered differently or not generated correctly
             print(f"Aviso: ID {news_id} n√£o encontrado em final_df_small_marca para adicionar ShortURL. Ignorando atualiza√ß√£o do ShortURL neste DF.")
             # Optionally, add the news info to final_df_small_marca if it's missing
             # new_row = news_info_marca.copy()
             # new_row['ShortURL'] = short_url_marca
             # final_df_small_marca = pd.concat([final_df_small_marca, pd.DataFrame([new_row])], ignore_index=True)


        # 7. Criar a string formatada

        # Incluir trecho de c√≥digo para identificar o tipo "Special"
        special_type = ""
        # Busca o Id no final_df_SPECIALS_small
        special_info = final_df_SPECIALS_small[final_df_SPECIALS_small['Id'] == news_id]

        if not special_info.empty:
            # Se encontrar, verifica o campo Canais
            # Certifica-se de que 'Canais' √© uma lista ou string e a processa
            canais_special = special_info.iloc[0]['Canais']
            if isinstance(canais_special, list):
                 canais_str = ', '.join(map(str, canais_special)) # Converte lista para string
            else:
                 canais_str = str(canais_special) # Garante que √© string


            if "Editoriais" in canais_str:
                special_type = "Editorial"
            elif "Colunistas" in canais_str:
                special_type = "Colunista"
            elif "1¬™ P√°gina" in canais_str:
                special_type = "Capa"
            # Se n√£o contiver nenhuma das strings acima, special_type permanece ""

        # Formata a string incluindo o tipo "Special" se encontrado
        if special_type:
             group_string += f"{w_veiculo_marca} ({special_type} - {short_url_marca}), "
        else:
             group_string += f"{w_veiculo_marca} ({short_url_marca}), "


    # 8. Limpar e adicionar o resumo √† string
    # Remover a √∫ltima v√≠rgula e espa√ßo da string de ve√≠culos/urls
    group_string = group_string.rstrip(', ')
    group_string += "\n" # Adicionar quebra de linha antes do resumo

    # Check if 'Resumo' column exists and is not None
    if 'Resumo' not in row_marca or pd.isna(row_marca['Resumo']):
         print(f"Aviso: Linha {index} no df_resumo_marca n√£o tem Resumo. Adicionando placeholder.")
         resumo_limpo = "[Resumo n√£o dispon√≠vel]"
    else:
        resumo_limpo = str(row_marca['Resumo']) # Ensure it's string first
        # Remove the specific string and strip whitespace - This is handled in the final processing step now
        # resumo_limpo = resumo_limpo.replace("(160 palavras)", "").strip()


    group_string += resumo_limpo

    # 9. Adicionar a string ao documento DOCX
    document.add_paragraph(group_string)
    document.add_paragraph("")  # Adicionar linha em branco


# DEBUG: Check final_df_small_marca before sorting
print(f"\nVerificando final_df_small_marca antes de ordenar para links:")
print(f"  Tem {len(final_df_small_marca)} linhas.")
print(f"  Tem coluna 'Canais'? {'Canais' in final_df_small_marca.columns}")

if not final_df_small_marca.empty and 'Canais' in final_df_small_marca.columns:
    # Sort the DataFrame
    # Assuming you still want the specific order defined by marca1, marca2, w_marcas
    # Ensure marca1, marca2, w_marcas are defined before this block
    # If not, a simple sort_values('Canais') will work but might not match your desired order
    try:
        # Attempt the custom sort if order is defined
        # Ensure marca1, marca2, w_marcas are accessible here (defined earlier in the notebook)
        order = [marca1, marca2] + [marca for marca in final_df_small_marca['Canais'].unique() if marca not in (marca1, marca2)]
        final_df_small_marca_sorted = final_df_small_marca.sort_values(by=['Canais'], key=lambda x: pd.Categorical(x, categories=order, ordered=True))
        print("Ordena√ß√£o personalizada dos links por Marca aplicada.")
    except (NameError, KeyError, AttributeError) as e:
        print(f"Aviso: Falha na ordena√ß√£o personalizada dos links por Marca ({type(e).__name__}: {e}). Verifique se marca1, marca2 e w_marcas est√£o definidos corretamente.")
        print("Ordenando por Canais padr√£o.")
        final_df_small_marca_sorted = final_df_small_marca.sort_values(by=['Canais'])


    # Adicionar uma quebra de linha entre a se√ß√£o de Setor e a se√ß√£o de links de Marca (se houver links de Marca)
    document.add_paragraph("")
    document.add_paragraph("--- Links das Not√≠cias de Marca ---") # Opcional: um t√≠tulo para a se√ß√£o de links
    document.add_paragraph("")

    current_marca = None  # Inicializar vari√°vel para controlar a marca atual

    for index, row_small_marca in final_df_small_marca_sorted.iterrows(): # Use the sorted DataFrame
        marca = row_small_marca['Canais']
        if marca != current_marca:  # Verificar se a marca mudou
            if current_marca is not None:  # Add blank line if not the first brand
                document.add_paragraph("")
            document.add_paragraph(f"*{marca}*")  # Incluir nome da marca
            current_marca = marca

        # Obter informa√ß√µes do final_df_marca (original dataframe) para Veiculo, Titulo, CanaisCommodities
        # Assumindo que final_df_marca e final_df_small_marca compartilham o mesmo 'Id'
        news_id_small = row_small_marca['Id']
        original_row_marca = final_df_marca[final_df_marca['Id'] == news_id_small]
        if original_row_marca.empty:
             print(f"Aviso: ID {news_id_small} n√£o encontrado em final_df_marca para links de Marca. Pulando este link.")
             continue # Skip this link if original info not found

        original_row_marca = original_row_marca.iloc[0]

        veiculo = original_row_marca['Veiculo']
        titulo = original_row_marca['Titulo'] # Use titulo from original df
        # Verifique se 'CanaisCommodities' existe no seu final_df_marca
        canais_commodities = original_row_marca.get('CanaisCommodities', '') # Usa .get para evitar erro se a coluna n√£o existir


        # Incluir linha com Veiculo e Titulo
        document.add_paragraph(f"{veiculo}: {titulo}") # Use 'titulo' variable

        # Incluir linha com ShortURL e Coluna (se aplic√°vel)
        # Ensure 'ShortURL' column exists in final_df_small_marca
        short_url = row_small_marca.get('ShortURL', original_row_marca.get('UrlVisualizacao', 'URL N√£o Encontrada')) # Get ShortURL from small DF or fallback to original URL

        prefix = "Coluna - " if "Colunistas" in str(canais_commodities) else ""
        document.add_paragraph(f"{prefix}{short_url}")

        # Check if it's the last news item for the current brand
        # Check against the sorted DataFrame
        # Ensure we don't go out of bounds
        if index + 1 < len(final_df_small_marca_sorted):
             if final_df_small_marca_sorted.iloc[index + 1]['Canais'] == marca:
                # If not the last item, add the asterisk line
                document.add_paragraph("*")
        # If it is the last item for this brand, the next iteration will add a blank line if needed.
else:
    print("DataFrame 'final_df_small_marca' n√£o encontrado, vazio ou sem a coluna 'Canais'. Pulando a se√ß√£o de links por Marcas.")


# --- Novo Passo: Incluir resumos do Setor ---
# DEBUG: Check if df_resumo_setor is populated
print(f"\nVerificando df_resumo_setor:")
print(f"  Tem {len(df_resumo_setor)} linhas.")
print(f"  Est√° vazio? {df_resumo_setor.empty}")
if not df_resumo_setor.empty:
    document.add_paragraph("")
    document.add_paragraph("--- Not√≠cias de Setor ---") # Opcional: um t√≠tulo para a se√ß√£o de setor
    document.add_paragraph("")


# 10. Iterar o dataframe arq_results_setor (j√° deve estar ordenado por Tema)
current_tema = None  # Inicializar vari√°vel para controlar o tema atual

# Use df_resumo_setor, not arq_results_setor
for index, row_setor in df_resumo_setor.iterrows():
    # Check if 'Tema' column exists and is not None
    if 'Tema' not in row_setor or pd.isna(row_setor['Tema']):
        print(f"Aviso: Linha {index} no df_resumo_setor n√£o tem Tema. Pulando.")
        continue

    tema = row_setor['Tema']

    # 11. Quando iniciar um Tema novo, incluir uma linha com o conte√∫do do campo Tema, precedido e sucedido por um asterisco.
    if tema != current_tema:
        if current_tema is not None: # Adicionar linha em branco entre temas
            document.add_paragraph("")
        document.add_paragraph(f"*{tema}*")
        current_tema = tema

    # 12. Buscar informa√ß√µes no final_df_setor usando o Id
    # Check if 'Id' column exists and is not None
    if 'Id' not in row_setor or pd.isna(row_setor['Id']):
        print(f"Aviso: Linha {index} no df_resumo_setor n√£o tem Id v√°lido. Pulando.")
        continue

    try:
        news_id = int(str(row_setor['Id']).strip()) # Convert to integer, ensure string then strip
    except ValueError:
        print(f"Aviso: N√£o foi poss√≠vel converter ID '{row_setor['Id']}' para inteiro na linha {index} de df_resumo_setor. Pulando.")
        continue # Skip this ID if not a valid number

    news_info_setor = final_df_setor[final_df_setor['Id'] == news_id]
    if news_info_setor.empty:
        print(f"Aviso: ID {news_id} n√£o encontrado em final_df_setor para resumo de Setor. Pulando.")
        continue # Skip this news if not found
    news_info_setor = news_info_setor.iloc[0]

    w_veiculo_setor = news_info_setor['Veiculo']
    w_titulo_setor = news_info_setor['Titulo']
    w_url_setor = news_info_setor['UrlVisualizacao']

    # 13. Incluir linha com Veiculo e Titulo
    document.add_paragraph(f"{w_veiculo_setor}: {w_titulo_setor}")

    # 14. Incluir o Resumo do Setor
    # Check if 'Resumo' column exists and is not None
    if 'Resumo' not in row_setor or pd.isna(row_setor['Resumo']):
        print(f"Aviso: Linha {index} no df_resumo_setor (Tema {tema}) n√£o tem Resumo. Adicionando placeholder.")
        document.add_paragraph("[Resumo n√£o dispon√≠vel]")
    else:
        resumo_bruto = str(row_setor['Resumo'])
        # Remove prefix like "**Resumo (90 palavras):**" or "**Resumo (160 palavras):**"
        # This pattern is now covered by the more general removal later.
        # Removing specific prefixes here might be redundant or interfere with the final step.
        # Keeping the raw resumo to be processed in the final cleanup loop.
        document.add_paragraph(resumo_bruto) # Add raw resumo, will be processed later


    # 15. Incluir a URL encurtada (Encapsular a l√≥gica de encurtamento)
    s = pyshorteners.Shortener()
    retries = 3
    short_url_setor = w_url_setor # Inicializa com a URL original
    for i in range(retries):
        try:
            short_url_setor = s.tinyurl.short(w_url_setor)
            break
        except requests.exceptions.RequestException as e:
            # Check if the error is due to a redirect or other network issue
            if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                print(f"Erro de conex√£o/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Setor ID {news_id}: {e}")
            else:
                print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Setor ID {news_id}: {e}")
            if i < retries - 1:
                time.sleep(2)
            else:
                print(f"Falha ao encurtar URL para Setor ID {news_id} ap√≥s {retries} tentativas. Usando URL original.")

    document.add_paragraph(short_url_setor)

    # 16. Incluir a linha com um asterisco
    document.add_paragraph("*")



# --- Novo Passo: Incluir Editoriais ---
print(f"\nProcessando {len(final_df_editorial)} editoriais...")
if not final_df_editorial.empty:
    print('Entrei aqui...')
    document.add_paragraph("")
    document.add_paragraph("*EDITORIAIS*")
    document.add_paragraph("") # Linha em branco ap√≥s o t√≠tulo dos editoriais

    for index, row_editorial in final_df_editorial.iterrows():
        # 17. Incluir uma linha com o conte√∫do do campo Veiculo, mais a string ": ", mais o conte√∫do do campo Conteudo
        w_veiculo_editorial = row_editorial.get('Veiculo', 'Ve√≠culo Desconhecido')
        w_conteudo_editorial = row_editorial.get('Titulo', 'T√≠tulo n√£o dispon√≠vel')
        document.add_paragraph(f"{w_veiculo_editorial}: {w_conteudo_editorial}")

        # 18. Incluir uma linha com a url encurtada do conte√∫do do campo UrlVisualizacao
        w_url_editorial = row_editorial.get('UrlVisualizacao', 'URL N√£o Dispon√≠vel')

        s = pyshorteners.Shortener()
        retries = 3
        short_url_editorial = w_url_editorial
        for i in range(retries):
            try:
                short_url_editorial = s.tinyurl.short(w_url_editorial)
                break
            except requests.exceptions.RequestException as e:
                if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                    print(f"Erro de conex√£o/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Editorial {index}: {e}")
                else:
                    print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Editorial {index}: {e}")
                if i < retries - 1:
                    time.sleep(2)
                else:
                    print(f"Falha ao encurtar URL para Editorial {index} ap√≥s {retries} tentativas. Usando URL original.")

        document.add_paragraph(short_url_editorial)
        document.add_paragraph("*") # Adicionar linha com asterisco ap√≥s cada editorial



# ===== PROCESSAMENTO DO DOCUMENTO ANTES DA GRAVA√á√ÉO =====
# Este c√≥digo deve ser inserido logo antes da linha: document.save(arq_resumo_final)

print("\nProcessando documento antes da grava√ß√£o para remover padr√µes...")

paragraphs_to_remove_indices = []  # Lista de √≠ndices para remo√ß√£o

# Pattern for lines starting with "*(" and ending with "palavras)"
pattern_line_start_paren_end_palavras = r"^\s*\*\s*\(.*?palavras\)\s*$"
# Pattern for lines starting with "*Resumo"
pattern_line_start_resumo = r"^\s*\*Resumo.*$"
# Pattern for any occurrence of "(90 palavras)" or "(160 palavras)" etc.
# More specific: only matches patterns like "(NN palavras)" to avoid removing dates like "de 2026"
pattern_parenthesized_palavras_general = r"\s*\(\s*\d+\s*palavras\s*\)[\s\:\*]*"
# Add patterns for specific prefixes like "**Resumo:**" or "*Resumo:*"
pattern_specific_resumo_prefixes = r"^\s*[\*\s]*Resumo\s*[:\*\s]*"


for i, paragraph in enumerate(document.paragraphs):
    texto = paragraph.text

    # 1) Eliminar todas as linhas que come√ßam com a string "*(" E terminam com a string "palavras)"
    if re.fullmatch(pattern_line_start_paren_end_palavras, texto, re.IGNORECASE):
        paragraphs_to_remove_indices.append(i)
        print(f"  Removendo linha {i} por corresponder ao padr√£o '* (... palavras)'")
        continue # Skip further processing for this paragraph

    # 2) Eliminar as linhas que come√ßam com a string "*Resumo"
    if re.fullmatch(pattern_line_start_resumo, texto, re.IGNORECASE):
         paragraphs_to_remove_indices.append(i)
         print(f"  Removendo linha {i} por come√ßar com '*Resumo'")
         continue # Skip further processing for this paragraph


    # 3) Eliminar as strings que come√ßam com a string "*(", e que tenham algum conte√∫do na sequ√™ncia, e em seguida encerrem com a string "palavras)"
    # Also handle patterns like "**Resumo (...):**" and "*Resumo:*"
    new_text = re.sub(pattern_parenthesized_palavras_general, '', texto, flags=re.IGNORECASE).strip()
    new_text = re.sub(pattern_specific_resumo_prefixes, '', new_text, flags=re.IGNORECASE).strip()


    # If the text changed, update the paragraph
    if new_text != texto.strip(): # Compare with stripped original text
        paragraph.text = new_text
        print(f"  Removido padr√£o '(... palavras)' ou prefixos de resumo na linha {i}")

    # Replace any remaining "**" with "*"
    if "**" in paragraph.text:
         paragraph.text = paragraph.text.replace("**", "*")
         # print(f"  Substitu√≠do '**' por '*' na linha {i}") # Optional debug print


# Remover os par√°grafos marcados para remo√ß√£o (em ordem reversa)
for i in sorted(paragraphs_to_remove_indices, reverse=True):
    p = document.paragraphs[i]._element
    p.getparent().remove(p)

print(f"\nTotal de par√°grafos removidos: {len(paragraphs_to_remove_indices)}")


# 3. Agora salvar o documento processado
document.save(arq_resumo_final)

# 19. Salvar o documento DOCX
# document.save(arq_resumo_final) # J√° salvo acima
print(f"Arquivo DOCX salvo em: {arq_resumo_final}")

"""##Realce dos Hyperlinks - **Vers√£o Ajustada** (source: Claude)"""

# Instalar as bibliotecas necess√°rias no Google Colab
##!pip install python-docx

import re
from docx import Document
from docx.shared import RGBColor
from docx.oxml.shared import OxmlElement, qn

def adicionar_hyperlink(paragraph, url, texto_display):
    """
    Adiciona um hyperlink a um par√°grafo no documento Word
    """
    # Criar o elemento hyperlink
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), paragraph.part.relate_to(url,
                                                       "http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink",
                                                       is_external=True))

    # Criar o run com o texto do link
    new_run = OxmlElement('w:r')

    # Configurar propriedades do texto (cor azul, sublinhado)
    rPr = OxmlElement('w:rPr')

    # Cor azul
    color = OxmlElement('w:color')
    color.set(qn('w:val'), '0000FF')
    rPr.append(color)

    # Sublinhado
    underline = OxmlElement('w:u')
    underline.set(qn('w:val'), 'single')
    rPr.append(underline)

    new_run.append(rPr)

    # Adicionar o texto
    new_run.text = texto_display
    hyperlink.append(new_run)

    return hyperlink

def processar_urls_em_paragrafo(paragraph):
    """
    Processa um par√°grafo, convertendo URLs em hyperlinks
    """
    texto_completo = paragraph.text

    # REGEX CORRIGIDA - Captura URLs completas incluindo par√¢metros, fragmentos, etc.
    # Esta regex √© mais robusta e captura URLs at√© encontrar espa√ßo ou fim de linha
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    # Alternativa ainda mais simples e eficaz:
    # padrao_url = r'https?://\S+'

    urls_encontradas = re.findall(padrao_url, texto_completo)

    # Remover URLs duplicadas mantendo a ordem
    urls_unicas = []
    for url in urls_encontradas:
        if url not in urls_unicas:
            urls_unicas.append(url)

    if urls_unicas:
        print(f"   üîó Encontradas {len(urls_unicas)} URLs: {urls_unicas[:2]}{'...' if len(urls_unicas) > 2 else ''}")

        # Limpar o par√°grafo atual
        paragraph.clear()

        # Dividir o texto pelas URLs
        texto_restante = texto_completo

        for url in urls_unicas:
            # Encontrar a posi√ß√£o da URL no texto
            if url in texto_restante:
                partes = texto_restante.split(url, 1)

                if len(partes) == 2:
                    # Adicionar texto antes da URL
                    if partes[0]:
                        paragraph.add_run(partes[0])

                    # Adicionar hyperlink
                    hyperlink_element = adicionar_hyperlink(paragraph, url, url)
                    paragraph._p.append(hyperlink_element)

                    # Continuar com o resto do texto
                    texto_restante = partes[1]

        # Adicionar texto restante ap√≥s a √∫ltima URL
        if texto_restante:
            paragraph.add_run(texto_restante)

        return True

    return False

def converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida):
    """
    Converte URLs em hyperlinks em um arquivo DOCX
    """
    try:
        # Abrir o documento
        print(f"üìñ Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        urls_convertidas = 0
        paragrafos_processados = 0
        paragrafos_com_urls = 0

        # Processar todos os par√°grafos
        print("üîÑ Processando par√°grafos...")
        for i, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():  # Ignorar par√°grafos vazios
                if processar_urls_em_paragrafo(paragraph):
                    paragrafos_com_urls += 1
                paragrafos_processados += 1

                # Mostrar progresso a cada 25 par√°grafos
                if paragrafos_processados % 25 == 0:
                    print(f"   üìÑ Processados {paragrafos_processados} par√°grafos ({paragrafos_com_urls} com URLs)...")

        # Processar tabelas (se houver)
        print("üìä Processando tabelas...")
        tabelas_processadas = 0
        celulas_com_urls = 0

        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if paragraph.text.strip():
                            if processar_urls_em_paragrafo(paragraph):
                                celulas_com_urls += 1
            tabelas_processadas += 1

        # Salvar o documento processado
        print(f"üíæ Salvando arquivo: {arquivo_saida}")
        doc.save(arquivo_saida)

        print(f"\n‚úÖ Convers√£o conclu√≠da com sucesso!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Par√°grafos processados: {paragrafos_processados}")
        print(f"   - Par√°grafos com URLs: {paragrafos_com_urls}")
        print(f"   - Tabelas processadas: {tabelas_processadas}")
        print(f"   - C√©lulas com URLs: {celulas_com_urls}")
        print(f"   - Arquivo salvo como: {arquivo_saida}")

        return True

    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo '{arquivo_entrada}' n√£o encontrado!")
        print("Verifique se o arquivo existe no diret√≥rio atual.")
        return False

    except Exception as e:
        print(f"‚ùå Erro durante o processamento: {str(e)}")
        print(f"Detalhes do erro: {type(e).__name__}")
        return False

def metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida):
    """
    M√©todo alternativo melhorado - substitui URLs por texto com formata√ß√£o
    """
    try:
        print(f"üìñ M√©todo alternativo melhorado - Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        total_urls_encontradas = 0
        paragrafos_processados = 0

        # Regex melhorada para capturar URLs completas
        padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

        # Processar par√°grafos
        for paragraph in doc.paragraphs:
            if not paragraph.text.strip():
                continue

            texto_original = paragraph.text
            urls_no_texto = re.findall(padrao_url, texto_original)

            if urls_no_texto:
                print(f"   üîó Par√°grafo {paragrafos_processados + 1}: {len(urls_no_texto)} URLs encontradas")
                total_urls_encontradas += len(urls_no_texto)

                # Limpar o par√°grafo
                paragraph.clear()

                # Reconstruir o par√°grafo com formata√ß√£o
                texto_restante = texto_original

                for url in urls_no_texto:
                    if url in texto_restante:
                        # Dividir o texto pela URL
                        partes = texto_restante.split(url, 1)
                        if len(partes) == 2:
                            # Adicionar texto antes da URL
                            if partes[0]:
                                paragraph.add_run(partes[0])

                            # Adicionar URL com formata√ß√£o especial
                            run_url = paragraph.add_run(url)
                            run_url.font.color.rgb = RGBColor(0, 0, 255)  # Azul
                            run_url.underline = True

                            # Continuar com o resto
                            texto_restante = partes[1]

                # Adicionar texto restante
                if texto_restante:
                    paragraph.add_run(texto_restante)

            paragrafos_processados += 1

        # Processar tabelas tamb√©m
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if not paragraph.text.strip():
                            continue

                        texto_original = paragraph.text
                        urls_no_texto = re.findall(padrao_url, texto_original)

                        if urls_no_texto:
                            total_urls_encontradas += len(urls_no_texto)
                            paragraph.clear()

                            texto_restante = texto_original
                            for url in urls_no_texto:
                                if url in texto_restante:
                                    partes = texto_restante.split(url, 1)
                                    if len(partes) == 2:
                                        if partes[0]:
                                            paragraph.add_run(partes[0])

                                        run_url = paragraph.add_run(url)
                                        run_url.font.color.rgb = RGBColor(0, 0, 255)
                                        run_url.underline = True

                                        texto_restante = partes[1]

                            if texto_restante:
                                paragraph.add_run(texto_restante)

        # Salvar documento
        doc.save(arquivo_saida)

        print(f"\n‚úÖ M√©todo alternativo conclu√≠do!")
        print(f"üìä Estat√≠sticas:")
        print(f"   - Par√°grafos processados: {paragrafos_processados}")
        print(f"   - Total de URLs formatadas: {total_urls_encontradas}")
        print(f"üíæ Arquivo salvo como: {arquivo_saida}")

        return True

    except Exception as e:
        print(f"‚ùå Erro no m√©todo alternativo: {str(e)}")
        return False

def testar_regex():
    """
    Fun√ß√£o para testar a regex com URLs de exemplo
    """
    print("üß™ Testando regex com URLs de exemplo...")

    # URLs de teste
    urls_teste = [
        "https://tinyurl.com/2aymnjlf",
        "https://www.google.com/search?q=python",
        "http://example.com/path/to/file.html",
        "https://github.com/user/repo#readme",
        "https://site.com/page?param1=value1&param2=value2"
    ]

    # Regex melhorada
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    for url in urls_teste:
        match = re.search(padrao_url, url)
        if match:
            print(f"   ‚úÖ {url} -> Capturado: {match.group()}")
        else:
            print(f"   ‚ùå {url} -> N√£o capturado")

    print("\n" + "="*50)

# Fun√ß√£o principal para executar a convers√£o
def main():
    """
    Fun√ß√£o principal para executar a convers√£o
    """
    # Nomes dos arquivos (ajuste conforme necess√°rio)
    arquivo_entrada = arq_resumo_final  # Seu arquivo original
    arquivo_saida = arq_resumo_final_ajustado  # Arquivo com hyperlinks

    print("üöÄ Iniciando convers√£o de URLs para hyperlinks...")
    print("=" * 60)

    # Testar regex primeiro
    testar_regex()

    # Tentar primeiro m√©todo (hyperlinks verdadeiros)
    print("\nüîó Tentando m√©todo com hyperlinks verdadeiros...")
    sucesso = converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida)

    if not sucesso:
        print("\nüîÑ Tentando m√©todo alternativo melhorado...")
        print("-" * 50)
        arquivo_saida_alt = "arq_resumo_final_formatado.docx"
        sucesso = metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida_alt)

    if sucesso:
        print("\nüéâ Processo finalizado com sucesso!")
        print("\nüí° Dicas:")
        print("   - Abra o arquivo no Word para verificar os hyperlinks")
        print("   - Os links devem estar em azul e sublinhados")
        print("   - Teste alguns links clicando neles")
        print("   - Se algum link ainda estiver truncado, execute novamente")
    else:
        print("\n‚ùå N√£o foi poss√≠vel completar a convers√£o.")
        print("Verifique se o arquivo de entrada existe e n√£o est√° corrompido.")

# Fun√ß√£o de utilidade para listar arquivos DOCX no diret√≥rio atual
def listar_arquivos_docx():
    """
    Lista todos os arquivos .docx no diret√≥rio atual
    """
    import os
    arquivos_docx = [f for f in os.listdir('.') if f.endswith('.docx')]

    if arquivos_docx:
        print("üìÅ Arquivos .docx encontrados:")
        for i, arquivo in enumerate(arquivos_docx, 1):
            print(f"   {i}. {arquivo}")
    else:
        print("‚ùå Nenhum arquivo .docx encontrado no diret√≥rio atual.")

    return arquivos_docx

# Executar a convers√£o
if __name__ == "__main__":
    # Uncomment para listar arquivos dispon√≠veis primeiro
    # listar_arquivos_docx()
    main()

"""#Final do Cron√¥metro"""

# Fim do Cron√¥metro e C√°lculo do Tempo
end_time = datetime.datetime.now()
print(f"Fim do processamento: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

time_diff = end_time - start_time
minutes_spent = time_diff.total_seconds() / 60

print(f"Tempo gasto no processamento: {minutes_spent:.2f} minutos")