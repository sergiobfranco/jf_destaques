# -*- coding: utf-8 -*-
"""JF_Destaques_V2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uaqkZyBoRpx1boqI5EI6WmLN4bDgEdum

#**Destaques do Dia J&F**

"""
import os

# Nova definição do caminho base
JF_path = os.path.dirname(__file__)

#Temporizador

import datetime
import time
import pytz

# Função para solicitar e validar data e hora
def solicitar_data_hora_futura():
    while True:
        entrada = input("Digite a data e hora futuras no formato AAAA-MM-DD HH:MM:SS: ")
        try:
            data_hora_futura = datetime.datetime.strptime(entrada, "%Y-%m-%d %H:%M:%S")

            # Adicionar fuso horário de São Paulo
            fuso_horario_sp = pytz.timezone('America/Sao_Paulo')
            data_hora_futura_sp = fuso_horario_sp.localize(data_hora_futura)

            # Obter data e hora atual em SP
            data_hora_atual_sp = datetime.datetime.now(fuso_horario_sp)
            diferenca_tempo = data_hora_futura_sp - data_hora_atual_sp
            segundos_para_esperar = diferenca_tempo.total_seconds()

            if segundos_para_esperar < 0:
                print("A data e hora futuras especificadas já passaram.")
                continuar = input("Deseja continuar mesmo assim? (S/N): ").strip().upper()
                if continuar == "S":
                    return segundos_para_esperar
                else:
                    print("Vamos tentar novamente...\n")
                    continue
            else:
                return segundos_para_esperar

        except ValueError:
            print("Formato inválido. Use o formato AAAA-MM-DD HH:MM:SS.\n")

# Chamada da função
segundos_para_esperar = solicitar_data_hora_futura()

# Aguardar se for no futuro
if segundos_para_esperar > 0:
    horas = segundos_para_esperar / 60 / 60
    print(f"Faltam {horas:.2f} horas até a data e hora futuras especificadas em São Paulo.")
    print("Aguardando...")
    time.sleep(segundos_para_esperar)
    print("Espera terminada. O código continuará a execução.")
else:
    print("Continuando imediatamente pois a data especificada já passou.")


"""#Início do Cronômetro"""

import datetime

# Início do Cronômetro
start_time = datetime.datetime.now()
print(f"Início do processamento: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

"""#**V a r i á v e i s - G l o b a i s**"""

# Pastas
pasta_api = "dados/api"
pasta_marca_setor = "dados/marca_setor"

# Arquivo de Favoritos gerado pela API - ORIGINAL
favoritos_marca = "Favoritos_Marcas.xlsx"
arq_api_original = os.path.join(pasta_api, favoritos_marca)
aux = "aux.xlsx"
arq_aux = os.path.join(pasta_api, aux)

favoritos_setor = "Favoritos_Setor.xlsx"
arq_api_original_setor = os.path.join(pasta_api, favoritos_setor)

favoritos_setor_inter = "Favoritos_Setor_inter.xlsx"
arq_api_original_setor_inter = os.path.join(pasta_api, favoritos_setor_inter)

favoritos_editorial = "Favoritos_Editorial.xlsx"
arq_api_original_editorial = os.path.join(pasta_api, favoritos_editorial)

favoritos_SPECIALS = "Favoritos_SPECIALS.xlsx"
arq_api_original_SPECIALS = os.path.join(pasta_api, favoritos_SPECIALS)

# Arquivo de Favoritos gerado pela API - SMALL
favoritos_small_marca = "Favoritos_Marcas_small.xlsx"
arq_api = os.path.join(pasta_api, favoritos_small_marca)

favoritos_small_setor = "Favoritos_Setor_small.xlsx"
arq_api_setor = os.path.join(pasta_api, favoritos_small_setor)

favoritos_small_editorial = "Favoritos_Editorial_small.xlsx"
arq_api_editorial = os.path.join(pasta_api, favoritos_small_editorial)

favoritos_small_SPECIALS = "Favoritos_SPECIALS_small.xlsx"
arq_api_SPECIALS = os.path.join(pasta_api, favoritos_small_SPECIALS)

# Arquivo TXT de saída com os resumos
resumos = "resumos_marcas.txt"
arq_resumos = os.path.join(pasta_marca_setor, resumos)

# Arquivo XLSX de Notícias Similares
similares = "Grupos_Noticias_Similares.xlsx"
arq_similares = os.path.join(pasta_marca_setor, similares)

similares_setor = "Grupos_Noticias_Similares_Setor.xlsx"
arq_similares_setor = os.path.join(pasta_marca_setor, similares_setor)

# Arquivo XLSX de Prompt de Resumo
prompts = "Prompts_Resumo_Noticias.xlsx"
arq_prompts = os.path.join(pasta_marca_setor, prompts)

prompts_setor = "Prompts_Resumo_Noticias_Setor.xlsx"
arq_prompts_setor = os.path.join(pasta_marca_setor, prompts_setor)

# Arquivo XLSX de Resultados
results = "Resumos_Gerados_DeepSeek.xlsx"
arq_results = os.path.join(pasta_marca_setor, results)

results_final = "Resumos_Finais_DeepSeek.xlsx"
arq_results_final = os.path.join(pasta_marca_setor, results_final)


results_setor = "Resumos_Gerados_DeepSeek_Setor.xlsx"
arq_results_setor = os.path.join(pasta_marca_setor, results_setor)

# Arquivo DOCX de Resumos
resumo_final = "Resumo_Marcas.docx"
arq_resumo_final = os.path.join(pasta_marca_setor, resumo_final)

resumo_final_ajustado = "Resumo_Marcas_ajustado.docx"
arq_resumo_final_ajustado = os.path.join(pasta_marca_setor, resumo_final_ajustado)

# Marcas
w_marcas = ['Holding', 'J&F', 'JBS', 'Joesley Batista', 'Wesley Batista', 'Júnior Friboi', 'J&F Mineração/LHG', 'J&F Mineração/LHG Mining', \
            'Banco Original', 'PicPay', 'Eldorado', 'Flora', 'Âmbar Energia', 'Ambar Energia', \
            'Canal Rural', 'Braskem', 'Instituto J&F' ]
marcas_a_ignorar = ['J&F', 'JBS', 'Joesley Batista', 'Wesley Batista', 'Júnior Friboi', 'J&F Mineração/LHG', 'J&F Mineração/LHG Mining', \
                    'Banco Original', 'PicPay', 'Eldorado', 'Flora', 'Âmbar Energia', 'Ambar Energia', \
                    'Canal Rural', 'Braskem', 'Instituto J&F' ]
# Marcas que vêm em primeiro e segundo lugar no relatório
marca1 = 'J&F'
marca2 = 'JBS'

"""#Fast Track

# QUEBRA-GALHO
import requests
import pandas as pd
import os
import json
import re

final_df = pd.read_excel(arq_api_original)
final_df_small = pd.read_excel(arq_api)

final_df_SPECIALS_small = pd.read_excel(arq_api_SPECIALS)

final_df_setor = pd.read_excel(arq_api_original_setor)
final_df_setor_small = pd.read_excel(arq_api_setor)
"""

"""#Chamada da API - **MARCAS**"""

# Chamada da API para trazer os favoritos da MVC J&F Gestão (eliminando duplicatas de IdVeiculo + Titulo)
import requests
import pandas as pd
import os
import json
import re

# Caminho para o arquivo externo com as configurações da API
config_file = os.path.join(JF_path, "dados", "api_marca_configs.json")  # Nome do arquivo de configuração

# Carrega as configurações da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configurações da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se não, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisição POST para a API
    response = requests.post(url, json=data)
    print('response: ', response)
    # Verifique se a requisição foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame à lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisição para {url}: {response.status_code}")

# Concatena todos os DataFrames em um único DataFrame
final_df = pd.concat(all_dfs, ignore_index=True)

# Remover duplicatas de IdVeiculo + Titulo
# Convert 'DataVeiculacao' to datetime objects, coercing errors
final_df['DataVeiculacao'] = pd.to_datetime(final_df['DataVeiculacao'], errors='coerce')

# Sort by 'IdVeiculo', 'Titulo', and 'DataVeiculacao' in descending order
final_df = final_df.sort_values(by=['IdVeiculo', 'Titulo', 'DataVeiculacao'], ascending=[True, True, False])

# Remove duplicates based on 'IdVeiculo' and 'Titulo', keeping the first occurrence (which is the latest due to sorting)
final_df = final_df.drop_duplicates(subset=['IdVeiculo', 'Titulo'], keep='first').reset_index(drop=True)


# --- Início do ajuste ---
print('Qtde de registros antes de desprezar lista de veículos: ', final_df.shape[0])
# Lista de veículos a serem ignorados
veiculos_a_ignorar = [
    #"VALOR ECONÔMICO ONLINE/SÃO PAULO",
    #"CNN BRASIL ONLINE",
    #"VALOR INVESTE",
    #"ISTOÉ DINHEIRO ONLINE/SÃO PAULO",
    #"FOLHA DE S.PAULO ONLINE/SÃO PAULO",
    #"BLOOMBERG LÍNEA/AMÉRICA LATINA",
    #"RÁDIO CBN FM 90,5/SÃO PAULO",
    #"O POVO/FORTALEZA",
    #"O POVO ONLINE/FORTALEZA",
    "nenhum"
]

# Filtrar o DataFrame para remover as linhas onde a coluna 'Veiculo'
# está presente na lista de veículos a serem ignorados.
# Garante que a comparação seja case-insensitive e remova espaços em branco extras.
final_df = final_df[~final_df['Veiculo'].str.strip().str.upper().isin([v.strip().upper() for v in veiculos_a_ignorar])]

print('Qtde de registros após desprezar lista de veículos: ', final_df.shape[0])

# --- Fim do ajuste ---

# Substitui 'Holding' por 'J&F'

final_df['Canais'] = final_df['Canais'].fillna('').astype(str)
final_df['Canais'] = final_df['Canais'].str.replace(r'\bHolding\b', 'J&F', regex=True)

# Limpeza e ajustes nos Canais

def clean_canais(canais, w_marcas):
  """
  Remove colchetes e aspas, mantém apenas marcas em w_marcas,
  e remove vírgulas e espaços extras.
  """
  # 1. Converter para string se não for
  if not isinstance(canais, str):
    canais = str(canais)  # Converte para string

  # 2. Remover colchetes e aspas
  canais = re.sub(r"[\[\]']", "", canais)

  # 3. Manter apenas marcas em w_marcas
  marcas_validas = [marca for marca in canais.split(",") if marca.strip() in w_marcas]

  # 4. Remover vírgulas e espaços extras
  marcas_limpas = [marca.strip() for marca in marcas_validas]

  return ",".join(marcas_limpas)

# Aplicando a função ao DataFrame
final_df['Canais'] = final_df['Canais'].apply(lambda x: clean_canais(x, w_marcas))

# Replicar registros com vários Canais

# 1. Seleciona apenas as colunas de interesse
final_df_small = final_df[['Id', 'Titulo', 'Conteudo', 'IdVeiculo', 'Canais']].copy()

# 2. Explode os canais em listas
final_df_small['Canais'] = final_df_small['Canais'].str.split(',')

# 3. Replica as linhas
final_df_small = final_df_small.explode('Canais').copy()

# 4. Limpa espaços extras
final_df_small['Canais'] = final_df_small['Canais'].str.strip()

# Grava o DataFrame final em um arquivo Excel

final_df.to_excel(arq_api_original, index=False)
final_df_small.to_excel(arq_api, index=False)

print("Arquivo Excel salvo como ", arq_api_original)
print("Arquivo Excel salvo como ", arq_api)

"""#Avaliação de **RELEVÂNCIA**"""

# CÓDIGO ANTERIOR DO PROMPT DE RESUMO ADAPTADO POR MIM PARA
# Versão com avaliação de relevância da marca usando DeepSeek e agrupamento por similaridade com DBSCAN

import pandas as pd
import os
import requests
from dotenv import load_dotenv

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY não foi encontrada no arquivo .env")

# Carrega variável do Secrets no Google Colab
#if "DEEPSEEK_API_KEY" not in os.environ:
#    try:
#        load_dotenv()
#       API_KEY = os.getenv("DEEPSEEK_API_KEY")
#        os.environ["DEEPSEEK_API_KEY"] = userdata.get("DEEPSEEK_API_KEY")
#    except Exception as e:
#        raise RuntimeError("Erro ao acessar a chave DEEPSEEK_API_KEY nos Secrets do Colab.") from e

PROMPT_CHARACTER_LIMIT = 30000

arq_api = 'dados/api/Favoritos_Marcas_small.xlsx'
darq_relevancia_irrelevantes = 'dados/api/Favoritos_Marcas_Irrelevantes.xlsx' # Esta variável não será mais usada para salvar o arquivo
#arq_prompts = 'marca_setor/Prompts_Resumo_Noticias_DBSCAN.xlsx'

# DeepSeek config
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
#DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
}

# Carregamento inicial
df = pd.read_excel(arq_api)
df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')
marcas = df['Canais'].dropna().unique()

# Função para avaliar se a marca é relevante na notícia
def avaliar_relevancia_marca(marca, texto):
    prompt = (
        f"Avalie a relevância da marca \"{marca}\" na seguinte notícia.\n\n"
        "A marca deve ser considerada relevante quando influencia ou é influenciada pelos fatos descritos no texto, mesmo que de forma indireta ou moderada. "
        "Caso contrário, se a marca for apenas citada superficialmente, sem vínculo com os eventos principais, considere irrelevante.\n\n"
        "Responda apenas com 'True' (se for relevante) ou 'False' (se não for relevante).\n\n"
        f"Texto:\n{texto}"
    )

    data = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0,
        "max_tokens": 10
    }

    try:
        print(f"Enviando requisição ao DeepSeek para avaliar a marca '{marca}'...")
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        resposta = response.json()["choices"][0]["message"]["content"].strip()
        print(f"Resposta do DeepSeek: {resposta}")
        return resposta.lower().startswith("true")
    except Exception as e:
        print(f"Erro ao avaliar relevância: {e}")
        return True  # Em caso de erro, assume como relevante para não perder

# Avalia relevância se ainda não tiver sido feito
if 'RelevanciaMarca' not in df.columns or df['RelevanciaMarca'].isnull().all():
    print("Avaliando relevância da marca nas notícias...")
    df['RelevanciaMarca'] = df.apply(lambda row: avaliar_relevancia_marca(row['Canais'], row['TextoCompleto']), axis=1)

    # --- Início do ajuste para remover duplicatas relevantes ---
    # Define a ordem de prioridade das marcas
    # Removed duplicate 'Eldorado' from the list
    marca_order = ['JBS', 'J&F', 'PicPay', 'Eldorado', 'Joesley Batista', 'Wesley Batista', 'Banco Original']

    # Cria uma coluna temporária para ordenação personalizada
    df['Marca_Order'] = pd.Categorical(df['Canais'], categories=marca_order, ordered=True)

    # Filtra apenas as notícias relevantes (RelevanciaMarca == True) para aplicar a lógica de desduplicação
    df_relevantes = df[df['RelevanciaMarca'] == True].copy()

    # Ordena por Id e pela ordem de prioridade das marcas
    # Para IDs duplicados, a linha com a marca de maior prioridade (menor valor na categoria) virá primeiro
    df_relevantes_sorted = df_relevantes.sort_values(by=['Id', 'Marca_Order'], ascending=[True, True])

    # Remove duplicatas de Id, mantendo a primeira ocorrência (que será a de maior prioridade devido à ordenação)
    df_relevantes_deduplicadas = df_relevantes_sorted.drop_duplicates(subset='Id', keep='first')

    # Remove a coluna temporária de ordenação
    df_relevantes_deduplicadas = df_relevantes_deduplicadas.drop(columns=['Marca_Order'])

    # Separa as notícias irrelevantes do DataFrame original
    df_irrelevantes = df[df['RelevanciaMarca'] == False].copy()

    # Remove a coluna temporária de ordenação das irrelevantes também
    df_irrelevantes = df_irrelevantes.drop(columns=['Marca_Order'])


    # Concatena as notícias relevantes deduplicadas com as irrelevantes
    # A ordem das linhas não é garantida após a concatenação, mas as linhas corretas foram mantidas.
    # Se a ordem original for importante, pode ser necessário um passo adicional para reordenar.
    df_final_relevancia = pd.concat([df_relevantes_deduplicadas, df_irrelevantes], ignore_index=True)

    # --- NOVO PASSO: Remover notícias irrelevantes de df_final_relevancia ---
    print("Removendo notícias irrelevantes de df_final_relevancia...")
    df_final_relevancia = df_final_relevancia[df_final_relevancia['RelevanciaMarca'] == True].copy()
    print(f"df_final_relevancia agora contém {len(df_final_relevancia)} notícias relevantes.")
    # --- FIM NOVO PASSO ---


    # Salva o DataFrame final (agora apenas relevantes) no arquivo de relevância
    df_final_relevancia.to_excel("api/Favoritos_Marcas_Relevancia.xlsx", index=False)
    print("Arquivo api/Favoritos_Marcas_Relevancia.xlsx salvo (contém apenas notícias relevantes).")

    # --- Novo: Remover registros de final_df_small_marca que não estão em df_final_relevancia (Id e Canais) ---
    # Carregar final_df_small_marca para processamento
    final_df_small_marca = pd.read_excel(arq_api)

    # Criar um conjunto de tuplas (Id, Canais) das notícias que devem ser MANTIDAS (usando o df_final_relevancia já filtrado)
    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Filtrar final_df_small_marca para manter apenas as linhas cujos (Id, Canais) estão no conjunto
    # Ensure 'Canais' is treated as string in both DFs for consistent comparison
    final_df_small_marca['Canais'] = final_df_small_marca['Canais'].astype(str)
    df_final_relevancia['Canais'] = df_final_relevancia['Canais'].astype(str)

    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Apply the filter
    final_df_small_marca_processed = final_df_small_marca[
        final_df_small_marca.apply(lambda row: (row['Id'], row['Canais']) in ids_canais_to_keep, axis=1)
    ].copy()


    # Sobrescrever o arquivo Favoritos_Marcas_small.xlsx
    final_df_small_marca_processed.to_excel(arq_api, index=False)
    print(f"Arquivo {arq_api} sobrescrito após remoção de registros que não foram considerados relevantes/prioritários.")

    # --- Fim do novo ajuste ---


else:
    print("Coluna RelevanciaMarca já presente.")
    # Se a coluna já existe, mas o usuário quer aplicar a desduplicação,
    # você pode adicionar a lógica de desduplicação aqui também,
    # lendo o arquivo existente e re-salvando após a desduplicação.
    # Por simplicidade nesta resposta, apenas imprimimos a mensagem.
    # Para um comportamento mais robusto, a lógica de desduplicação
    # poderia ser aplicada sempre, ou ter uma flag para controlá-la.
    # Como a coluna RelevanciaMarca já existe, vamos carregar o df
    # e aplicar a lógica de desduplicação para garantir que o arquivo
    # de relevância esteja correto para as próximas etapas.

    print("Aplicando desduplicação em notícias relevantes do arquivo existente...")
    # Removed duplicate 'Eldorado' from the list
    marca_order = ['JBS', 'J&F', 'PicPay', 'Eldorado', 'Joesley Batista', 'Wesley Batista', 'Banco Original']
    df['Marca_Order'] = pd.Categorical(df['Canais'], categories=marca_order, ordered=True)
    df_relevantes = df[df['RelevanciaMarca'] == True].copy()
    df_relevantes_sorted = df_relevantes.sort_values(by=['Id', 'Marca_Order'], ascending=[True, True])
    df_relevantes_deduplicadas = df_relevantes_sorted.drop_duplicates(subset='Id', keep='first')
    df_relevantes_deduplicadas = df_relevantes_deduplicadas.drop(columns=['Marca_Order'])

    df_irrelevantes = df[df['RelevanciaMarca'] == False].copy()
    df_irrelevantes = df_irrelevantes.drop(columns=['Marca_Order']) # Remove se existia antes ou foi criada

    df_final_relevancia = pd.concat([df_relevantes_deduplicadas, df_irrelevantes], ignore_index=True)

    # --- NOVO PASSO: Remover notícias irrelevantes de df_final_relevancia (no bloco else) ---
    print("Removendo notícias irrelevantes de df_final_relevancia (no bloco else)...")
    df_final_relevancia = df_final_relevancia[df_final_relevancia['RelevanciaMarca'] == True].copy()
    print(f"df_final_relevancia agora contém {len(df_final_relevancia)} notícias relevantes (no bloco else).")
    # --- FIM NOVO PASSO ---

    df_final_relevancia.to_excel("api/Favoritos_Marcas_Relevancia.xlsx", index=False)
    print("Arquivo api/Favoritos_Marcas_Relevancia.xlsx re-salvo após desduplicação (contém apenas notícias relevantes).")

    # --- Novo: Remover registros de final_df_small_marca que não estão em df_final_relevancia (Id e Canais) (caso a coluna já existisse) ---
    # Carregar final_df_small_marca para processamento
    final_df_small_marca = pd.read_excel(arq_api)

    # Criar um conjunto de tuplas (Id, Canais) das notícias que devem ser MANTIDAS (usando o df_final_relevancia já filtrado)
    # Ensure 'Canais' is treated as string in both DFs for consistent comparison
    final_df_small_marca['Canais'] = final_df_small_marca['Canais'].astype(str)
    df_final_relevancia['Canais'] = df_final_relevancia['Canais'].astype(str)

    ids_canais_to_keep = set(zip(df_final_relevancia['Id'], df_final_relevancia['Canais']))

    # Apply the filter
    final_df_small_marca_processed = final_df_small_marca[
        final_df_small_marca.apply(lambda row: (row['Id'], row['Canais']) in ids_canais_to_keep, axis=1)
    ].copy()

    # Sobrescrever o arquivo Favoritos_Marcas_small.xlsx
    final_df_small_marca_processed.to_excel(arq_api, index=False)
    print(f"Arquivo {arq_api} sobrescrito após remoção de registros que não foram considerados relevantes/prioritários (coluna RelevanciaMarca já existia).")
    # --- Fim do novo ajuste (caso a coluna já existisse) ---


# --- Fim do ajuste para remover duplicatas relevantes ---

"""##Refinamento: Verificar se os **RESUMOS** tratam do **MESMO ASSUNTO** - **VERSAO 2**"""

# Etapa 2: Resumo de até 40 palavras, agrupamento semântico e geração de resumos finais com refinamento por subtemas

import pandas as pd
import os
import requests
import re

from dotenv import load_dotenv

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY não foi encontrada no arquivo .env")

#if "DEEPSEEK_API_KEY" not in os.environ:
#    try:
#        from google.colab import userdata
#        os.environ["DEEPSEEK_API_KEY"] = userdata.get("DEEPSEEK_API_KEY")
#    except Exception as e:
#        raise RuntimeError("Erro ao acessar a chave DEEPSEEK_API_KEY nos Secrets do Colab.") from e

arq_textos = "dados/api/Favoritos_Marcas_small.xlsx"
arq_saida_final = "dados/marca_setor/Resumos_Finais_DeepSeek.xlsx"

DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
#DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
}

def gerar_resumo_40(texto, id_):
    print(f"📝 Gerando resumo curto para notícia ID: {id_}...")
    prompt = "Resuma o conteúdo a seguir em até 40 palavras.\n\n" + texto
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 100
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"Erro ao gerar resumo curto para ID {id_}: {e}")
        return ""

def agrupar_por_similaridade(resumos):
    print(f"🔗 Enviando {len(resumos)} resumos para agrupamento semântico via DeepSeek...")
    prompt = (
        "Agrupe os resumos abaixo por similaridade de assunto. "
        "Considere como similar não apenas assuntos idênticos, mas também aqueles com forte relação temática, como diferentes aspectos de um mesmo setor, empresa ou impacto.\n"
        "Retorne uma única linha com os números dos grupos separados por vírgula, na mesma ordem dos resumos.\n"
        "Exemplo: 1,1,2,2,3\n\n"
    )
    for i, resumo in enumerate(resumos):
        prompt += f"Resumo {i+1}: {resumo}\n"
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 400
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"].strip()
        print("📤 Resposta bruta do agrupamento:")
        print(content)
        linha_grupos = next((l for l in content.splitlines() if re.match(r"^\d+(,\d+)*$", l.strip())), "")
        if not linha_grupos:
            print("⚠️ Nenhuma linha de grupo reconhecida. Conteúdo retornado:")
            for linha in content.splitlines():
                print(f"> {linha}")
        else:
            print(f"✅ Linha de grupos detectada: {linha_grupos}")
        grupos = [int(g) for g in linha_grupos.strip().split(",")] if linha_grupos else list(range(len(resumos)))
        if len(grupos) != len(resumos):
            print("⚠️ Número de grupos não bate. Atribuindo grupos únicos...")
            return list(range(len(resumos)))
        return grupos
    except Exception as e:
        print(f"Erro ao agrupar resumos: {e}")
        return list(range(len(resumos)))

def gerar_resumo_160(textos, marca):
    corpo = "\n--- NOTÍCIA ---\n".join(textos)
    prompt = f"Gere um resumo único de até 160 palavras para as notícias a seguir sobre a marca '{marca}', destacando os fatos mais importantes:\n\n{corpo}"
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_tokens": 400
    }
    try:
        response = requests.post(DEEPSEEK_API_URL, headers=HEADERS, json=data)
        response.raise_for_status()
        texto = response.json()["choices"][0]["message"]["content"].strip()
        texto = re.sub(r"^\*\*?Resumo.*?\*\*?\s*", "", texto, flags=re.IGNORECASE)
        texto = re.sub(r"\*\(160 palavras\)\*|\*\(Exatamente 160 palavras\)\*", "", texto, flags=re.IGNORECASE)
        texto = re.sub(r"\n{2,}", "\n", texto).strip()
        return texto
    except Exception as e:
        print(f"Erro ao gerar resumo final: {e}")
        return ""

def reavaliar_grupo(grupo_df):
    resumos = grupo_df['Resumo40'].tolist()
    ids = grupo_df['Id'].tolist()
    subgrupos = agrupar_por_similaridade(resumos)
    grupo_df = grupo_df.copy()
    grupo_df['SubGrupoID'] = subgrupos
    return grupo_df

try:
    df = pd.read_excel(arq_textos)
    df['Id'] = df['Id'].astype(str)
    df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')

    lista_resumos = []
    for _, row in df.iterrows():
        resumo = gerar_resumo_40(row['TextoCompleto'], row['Id'])
        lista_resumos.append({"Id": row['Id'], "Marca": row['Canais'], "TextoCompleto": row['TextoCompleto'], "Resumo40": resumo})

    df_resumos = pd.DataFrame(lista_resumos)
    grupos_iniciais = agrupar_por_similaridade(df_resumos['Resumo40'].tolist())
    df_resumos['GrupoID'] = grupos_iniciais

    refinado = []
    for (marca, grupo), grupo_df in df_resumos.groupby(['Marca', 'GrupoID']):
        if len(grupo_df) > 1:
            print(f"🔍 Reavaliando grupo {grupo} da marca {marca}...")
            grupo_df = reavaliar_grupo(grupo_df)
        else:
            grupo_df['SubGrupoID'] = 0
        refinado.append(grupo_df)

    df_refinado = pd.concat(refinado).reset_index(drop=True)

    resultados = []
    for (marca, grupo, subgrupo), df_sub in df_refinado.groupby(['Marca', 'GrupoID', 'SubGrupoID']):
        textos = df_sub['TextoCompleto'].tolist()
        ids = df_sub['Id'].tolist()
        resumo_final = gerar_resumo_160(textos, marca)
        resultados.append({
            "Marca": marca,
            "GrupoID": f"{marca}_G{grupo}_S{subgrupo}",
            "QtdNoticias": len(ids),
            "Ids": ','.join(ids),
            "Resumo": resumo_final
        })

    df_final = pd.DataFrame(resultados)
    df_final.to_excel(arq_saida_final, index=False)
    print(f"✅ Resumos finais salvos em {arq_saida_final}")

except Exception as e:
    print(f"Erro geral no processamento: {e}")



"""#Chamada da API - **SETOR**"""


# Chamada da API para trazer os favoritos da MVC Admin Impressos TOP 4 J&F Hoje
import requests
import pandas as pd
import os
import json
import re

# Caminho para o arquivo externo com as configurações da API
config_file = os.path.join(JF_path, "dados", "api_setor_configs.json")  # Nome do arquivo de configuração

# Carrega as configurações da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configurações da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se não, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisição POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisição foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame à lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisição para {url}: {response.status_code}")

# Concatena todos os DataFrames em um único DataFrame
final_df_setor = pd.concat(all_dfs, ignore_index=True)

# --- Added code to remove duplicates based on IdVeiculo + Titulo + DataVeiculacao ---
# Remover duplicatas de IdVeiculo + Titulo
# Convert 'DataVeiculacao' to datetime objects, coercing errors
final_df_setor['DataVeiculacao'] = pd.to_datetime(final_df_setor['DataVeiculacao'], errors='coerce')

# Sort by 'IdVeiculo', 'Titulo', and 'DataVeiculacao' in descending order
final_df_setor = final_df_setor.sort_values(by=['IdVeiculo', 'Titulo', 'DataVeiculacao'], ascending=[True, True, False])

# Remove duplicates based on 'IdVeiculo' and 'Titulo', keeping the first occurrence (which is the latest due to sorting)
final_df_setor = final_df_setor.drop_duplicates(subset=['IdVeiculo', 'Titulo'], keep='first').reset_index(drop=True)
# --- End of added code ---


# Variável com os títulos a ignorar
titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "horóscopo", \
                     "mensagens", "MIRIAM LEITÃO", "MÔNICA BERGAMO", "multitela", "Obituário", \
                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
                     "coluna do estadão", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_setor['Titulo'] = final_df_setor['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo começa com os termos a ignorar (comparação em minúsculas)
# Criar uma máscara booleana para as linhas a serem mantidas
mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a máscara para remover as linhas indesejadas
final_df_setor = final_df_setor[mask].copy()

# --- Added code to remove illegal characters ---
# Function to remove illegal characters
def remove_illegal_chars(text):
    if isinstance(text, str):
        # Remove characters not allowed in XML (and thus often in Excel)
        # This regex removes characters in the range U+0000 to U+0008, U+000B, U+000C, U+000E to U+001F
        illegal_chars = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')
        return illegal_chars.sub('', text)
    return text

# Apply the function to the 'Conteudo' column
final_df_setor['Conteudo'] = final_df_setor['Conteudo'].apply(remove_illegal_chars)
# --- End of added code ---

final_df_setor.to_excel(arq_api_original_setor_inter, index=False)

print("Arquivo Excel salvo como ", arq_api_original_setor)

# ↓↓↓↓↓↓↓↓↓↓ INÍCIO DO TRECHO PARA DESPREZAR REGISTROS DO ARQUIVO DE SETOR ↓↓↓↓↓↓↓↓↓↓↓↓↓↓

# Assuming final_df_setor and w_marcas are already defined in your environment

# 1. Eliminar registros com Secao indesejada (em minúsculas)
secoes_a_ignorar = ["esportes", "cotidiano", "folha corrida", "rio", "saúde", "opinião", "na web", \
                    "classificados", "cultura", "ilustrada"]

# Create a list of the lowercased and stripped versions of the sections to ignore
secoes_a_ignorar_cleaned = [s.strip().lower() for s in secoes_a_ignorar]

# Clean the 'Secao' column by stripping whitespace and converting to lowercase
final_df_setor['Secao_cleaned'] = final_df_setor['Secao'].astype(str).str.strip().str.lower()

# Now apply the filter using the cleaned column and the cleaned list of sections to ignore
final_df_setor_filtered = final_df_setor[
    ~final_df_setor['Secao_cleaned'].isin(secoes_a_ignorar_cleaned)
].copy()

# You can drop the temporary 'Secao_cleaned' column if you don't need it later
final_df_setor_filtered = final_df_setor_filtered.drop(columns=['Secao_cleaned'])

# 2. Eliminar registros cujo campo CanaisCommodities atenda às novas condições
# Condição para CanaisCommodities conter "Obituários"
condition_contains_obituarios = final_df_setor_filtered['CanaisCommodities'].astype(str).str.lower().str.contains('obituários', na=False)

# Combinar as condições de exclusão: (contém Obituários)
mask_exclude_canais = condition_contains_obituarios

# Filtrar o DataFrame para manter as linhas que NÃO atendem às condições de exclusão
final_df_setor_filtered = final_df_setor_filtered[~mask_exclude_canais].copy()

# 3. Eliminar registros cujo campo Conteudo contiver qualquer um dos termos constantes da variável marcas_a_ignorar (comparar em minúsculas)
# Criar um padrão regex para buscar qualquer uma das palavras em marcas_a_ignorar, com boundary words
marcas_a_ignorar_lower = [marca.lower() for marca in marcas_a_ignorar]
pattern_marcas_a_ignorar = r'\b(' + '|'.join(re.escape(marca) for marca in marcas_a_ignorar_lower) + r')\b'

# --- Modified this line to use the already cleaned 'Conteudo' column ---
final_df_setor_filtered = final_df_setor_filtered[
    ~final_df_setor_filtered['Conteudo'].str.lower().str.contains(pattern_marcas_a_ignorar, na=False)
].copy()
# --- End of modification ---

# 4. Eliminar registros onde no campo Conteudo tenha, numa mesma linha,
# que comece com "leia mais" ou "leia também", e em outro lugar qualquer
# da mesma linha, contiver qualquer um dos termos constantes da variável marcas_a_ignorar (comparar em minúsculas)

# Primeiro, lidar com valores NaN na coluna 'Conteudo'
final_df_setor_filtered['Conteudo'] = final_df_setor_filtered['Conteudo'].fillna('')

# Função para verificar a condição combinada em cada linha
def check_leia_mais_and_marcas_a_ignorar(content, marcas_a_ignorar):
    content_lower = content.lower()
    starts_with_leia = content_lower.strip().startswith("leia mais") or content_lower.strip().startswith("leia também")

    # Check if any of the marcas_a_ignorar are present in the lowercased content
    marca_present = any(re.search(r'\b' + re.escape(marca.lower()) + r'\b', content_lower) for marca in marcas_a_ignorar)

    return starts_with_leia and marca_present

# Aplicar a função para criar uma máscara booleana
# --- Modified this line to use the already cleaned 'Conteudo' column ---
mask_leia_mais_and_marcas_a_ignorar = final_df_setor_filtered['Conteudo'].apply(
    lambda x: check_leia_mais_and_marcas_a_ignorar(x, marcas_a_ignorar)
)
# --- End of modification ---

# Filtrar o DataFrame para remover as linhas que correspondem à condição
final_df_setor_filtered = final_df_setor_filtered[~mask_leia_mais_and_marcas_a_ignorar].copy()

# O DataFrame 'final_df_setor_filtered' agora contém os dados após todas as filtragens.


print(f"Número de registros antes da filtragem: {len(final_df_setor)}")
print(f"Número de registros após a filtragem: {len(final_df_setor_filtered)}")

# Você pode renomeá-lo para final_df_setor se quiser substituir o original.
final_df_setor = final_df_setor_filtered.copy()

# ↑↑↑↑↑↑↑↑↑↑ FINAL DO TRECHO PARA DESPREZAR REGISTROS DO ARQUIVO DE SETOR ↑↑↑↑↑↑↑↑↑↑↑↑↑↑


# Grava o DataFrame final em um arquivo Excel

final_df_setor.to_excel(arq_api_original_setor, index=False)

# Criar o DataFrame 'final_df_small' após a filtragem
final_df_setor_small = final_df_setor[['Id', 'Titulo', 'Conteudo', 'IdVeiculo']].copy()


final_df_setor_small.to_excel(arq_api_setor, index=False)

print("Arquivo Excel salvo como ", arq_api_original_setor)
print("Arquivo Excel salvo como ", arq_api_setor)

"""#Gerar Grupos e Prompts de Resumo - **SETOR**"""

#import pandas as pd
from sentence_transformers import SentenceTransformer, util
from sklearn.cluster import DBSCAN
import numpy as np
import re # Importar re para limpeza do tema
from collections import Counter # Importar Counter para contagem de termos

# 1. Carrega os dados (todas as notícias)
df = pd.read_excel(arq_api_setor)
df['TextoCompleto'] = df['Titulo'].fillna('') + '. ' + df['Conteudo'].fillna('')

# 2. Carrega o modelo de embeddings (ainda não usaremos para filtragem, mas pode ser útil para outras análises)
# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# 3. Identificar notícias relacionadas aos temas (usando frequência de termos) e calcular a pontuação de relevância
temas_termos = {
    "Setor de Papel e Celulose": ["papel", "celulose", "fibra", "eucalipto", "pulp", "paper", "cellulose"],
    "Setor de Mineração": ["mineração", "mineradora", "minério", "ferro", "níquel", "ouro", "metal", "geologia", "jazida", "mina", "mining"],
    "Setor de Agronegócios": ["agronegócio", "agro", "pecuária", "lavoura", "safra", "colheita", "grãos", "soja", "milho", "carne", "laranjas", \
                              "exportação agrícola", "rural", "agribusiness", "gripe aviária", "avícolas", "derivados", "frango", "ovos", \
                              "cacau", "crédito rural", "h5n1", "rastreabilidade", "tecnologia agrícola", "inovação agrícola", "caprinos", \
                              "ovinos", "abpa"],
    "Setor de Educação": ["educação", "escola", "universidade", "ensino", "aluno", "professor", "faculdade", "curso", "vestibular", \
                          "enem", "educacional", "estudante", "estudantes", "vestibulares", "educacional", "educacionais", "docente", \
                          "aprendizagem", "ead", "mec"],
    "Setor de Energia": ["energia", "elétrica", "usina", "hidrelétrica", "termelétrica", "eólica", "solar", "transmissão", "distribuição", \
                         "gasolina", "diesel", "etanol", "combustível", "petróleo", "gás", "conta de luz", "mme"],
    "Setor de Finanças": ["finanças", "banco", "crédito", "investimento", "investimentos", "mercado financeiro", "mercados", "ação", "renda fixa", \
                          "câmbio", "dívida", "lucro", "capital", "IPO", "banco central", "política monetária", "política econômica", "governo", \
                          "ações", "fundos", "balanço", "balanços", "bolsa", "nasdaq", "etf", "tributação", "contribuinte", "selic", "juros", \
                          "precatórios", "inflação", "deficit fiscal", "ibs", "cbs", "ibovespa", "b3"],
    "Setor de Óleo de Gás": ["óleo", "gás", "petróleo", "exploração", "refinaria", "gasoduto", "poço", "onshore", "offshore", \
                             "glp", "petrobras", "anp"],
    "Justiça": ["justiça", "judiciário", "tribunal", "juiz", "ministério público", "processo", "sentença", "condenação", "advogado", \
                "lei", "legal", "stf", "supremo", "pena", "penas", "jurisprudência", "pgr", "julgamento", "recurso", "judicial", \
                "denúncia", "acusação", "stj", "cnj", "golpe de estado", "penduricalhos", "agu", "alexandre de moraes"],
    "Meio Ambiente e ESG": ["meio ambiente", "sustentabilidade", "ambiental", "ambientalistas", "ecologia", "desmatamento", \
                            "poluição", "clima", "ESG", "governança ambiental", "responsabilidade social", "emissão de carbono", \
                            "biodiversidade", "amazônia", "floresta", "exploração", "cerrado", "mata atlântica", "cop30", \
                            "licenciamento ambiental", "créditos de carbono", "ibama"],
    "Política - Governo e Congresso Nacional": ["política", "governo", "congresso", "eleição", "eleições", "reeleição", "partido", "partidos", \
                                                "ministro", "ministra", "presidente", "ex-presidente", "senado", "câmara", "deputado", "deputada", \
                                                "senador", "senadora", "urnas", "executivo", "legislativo", "tse", "planalto", "primeira-dama", \
                                                "casa civil", "inss", "fraude", "cpmi", "trama golpista"],
    "Setor de Esportes": ["esporte", "futebol", "basquete", "vôlei", "atletismo", "olimpíadas", "copa", "campeonato", "clube", "jogador", \
                          "treinador", "partida", "competição", "cbf", "federação", "federações", "clubes", "atleta", "atletas", \
                          "arbitragem", "fifa", "xaud", "ednaldo"] # Novo setor adicionado
}

# Definir os IDs dos veículos prioritários
veiculos_prioritarios = [10459, 675]
pontuacao_extra_veiculo = 100 # Pontuação extra para notícias desses veículos (ajuste este valor se necessário)

def calculate_relevance_score(text, id_veiculo, temas_termos, veiculos_prioritarios, pontuacao_extra_veiculo):
    """
    Calcula uma pontuação de relevância para a notícia.
    A pontuação é baseada na frequência dos termos chave dos temas, no tamanho do texto
    e em uma pontuação extra para veículos prioritários.
    Retorna a pontuação de relevância e o tema preponderante.
    """
    text_lower = text.lower()
    theme_counts = Counter()
    total_term_count = 0

    for tema, termos in temas_termos.items():
        for termo in termos:
            count = len(re.findall(r'\b' + re.escape(termo) + r'\b', text_lower))
            theme_counts[tema] += count
            total_term_count += count

    # Remover temas com contagem zero para identificar o tema preponderante
    theme_counts_filtered = {tema: count for tema, count in theme_counts.items() if count > 0}

    preponderant_theme = None
    if theme_counts_filtered:
        preponderant_theme = max(theme_counts_filtered, key=theme_counts_filtered.get)

    # Calcular o tamanho do texto (número de palavras)
    text_size = len(text.split())

    # Calcular a pontuação do veículo
    pontuacao_veiculo = pontuacao_extra_veiculo if id_veiculo in veiculos_prioritarios else 0

    # Calcular a pontuação total de relevância
    # Podemos simplesmente somar a contagem total de termos, o tamanho do texto e a pontuação do veículo.
    # A relação exata entre esses fatores pode ser ajustada se necessário.

    # Removi temporariamente o text_size e pontuacao_veiculo
    #relevance_score = total_term_count + text_size + pontuacao_veiculo

    relevance_score = total_term_count

    return relevance_score, preponderant_theme

# Aplicar a função para calcular a pontuação e identificar o tema preponderante
results = df.apply(lambda row: calculate_relevance_score(row['TextoCompleto'], row['IdVeiculo'], temas_termos, veiculos_prioritarios, pontuacao_extra_veiculo), axis=1)

# Separar os resultados em novas colunas
df['RelevanceScore'], df['TemaPreponderante'] = zip(*results)

# **Adicionar este passo para remover notícias do 'Setor de Esportes'**
#print("Número de notícias antes da filtragem: ", len(df))
#df_relevante = df[df['TemaPreponderante'] != 'Setor de Esportes'].copy()
#print("Número de notícias após a filtragem: ", len(df_relevante))

# Filtrar notícias que se enquadram em algum tema preponderante (ainda necessário)
#df_relevante = df[df['TemaPreponderante'].notna()].copy()

# Identificar notícias do 'Setor de Esportes'
df_esportes = df[df['TemaPreponderante'] == 'Setor de Esportes'].copy()

# Filtrar o DataFrame para remover notícias do 'Setor de Esportes' E notícias sem tema
df_relevante = df[
    (df['TemaPreponderante'] != 'Setor de Esportes') &  # Remove Setor de Esportes
    (df['TemaPreponderante'].notna())                 # Remove notícias sem tema
].copy()

print("Número de notícias antes da filtragem: ", len(df))
print(f"Número de notícias do Setor de Esportes removidas: {len(df_esportes)}")
print("Número de notícias após a filtragem: ", len(df_relevante))

# Definir os temas prioritários e a quantidade de notícias a serem selecionadas de cada um
temas_prioritarios = {
    "Política - Governo e Congresso Nacional": 8,
    "Setor de Finanças": 7,
    "Justiça": 5,
    "Setor de Agronegócios": 5
}

df_top_noticias_list = []

# Selecionar as top notícias dos temas prioritários
for tema, qtd in temas_prioritarios.items():
    df_tema = df_relevante[df_relevante['TemaPreponderante'] == tema].sort_values(by='RelevanceScore', ascending=False)
    df_top_noticias_list.append(df_tema.head(qtd))

# Filtrar os temas restantes
temas_restantes = [tema for tema in df_relevante['TemaPreponderante'].unique() if tema not in temas_prioritarios]
df_restantes = df_relevante[df_relevante['TemaPreponderante'].isin(temas_restantes)].copy()

# Selecionar as top 7 notícias dos temas restantes
if not df_restantes.empty:
    df_restantes_sorted = df_restantes.sort_values(by='RelevanceScore', ascending=False)
    df_top_noticias_list.append(df_restantes_sorted.head(7))

# Concatenar todos os DataFrames das notícias selecionadas
df_top_noticias = pd.concat(df_top_noticias_list, ignore_index=True)

# 6. Criar prompts para cada notícia selecionada
prompts = []
for index, row in df_top_noticias.iterrows():
    texto_noticia = row['TextoCompleto']
    tema_preponderante = row['TemaPreponderante']
    ids_noticia = str(row['Id']) # ID da notícia
    relevance_score = row['RelevanceScore'] # Obter a pontuação de relevância
    id_veiculo_noticia = row['IdVeiculo'] # Obter o IdVeiculo da notícia

    # Criar o prompt
    corpo = texto_noticia
    prompt = (
        f"Resuma a notícia abaixo em no máximo 90 palavras. "
        f"O resumo deve focar nos aspectos relacionados ao tema de {tema_preponderante}:\n\n"
        f"{corpo}"
    )

    prompts.append({
        "Ids": ids_noticia,
        "Tipo": "Notícia Individual",
        "Prompt": prompt,
        "Tema": tema_preponderante, # Adicionar coluna de Tema Preponderante
        "RelevanceScore": relevance_score, # Adicionar a pontuação de relevância
        "IdVeiculo": id_veiculo_noticia # Adicionar o IdVeiculo
    })

# 7. Salva em Excel
df_prompts = pd.DataFrame(prompts)

# Ordenar por tema para melhor organização (ou pela ordem que preferir para o arquivo de saída)
df_prompts = df_prompts.sort_values(by='Tema')

df_prompts.to_excel(arq_prompts_setor, index=False)
print("Arquivo salvo: ", arq_prompts_setor)

"""#Processar pelo DeepSeek - **SETOR**"""

##!pip install requests

#import pandas as pd
import requests
import time

# 1. Configuração

load_dotenv()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not DEEPSEEK_API_KEY:
    raise RuntimeError("Chave DEEPSEEK_API_KEY não foi encontrada no arquivo .env")

API_URL = "https://api.deepseek.com/v1/chat/completions"

HEADERS = {
    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
    "Content-Type": "application/json"
}

# 2. Carrega prompts gerados
df = pd.read_excel(arq_prompts_setor)

# 3. Função para chamar a API
def resumir_prompt(prompt_text):
    payload = {
#        "model": "deepseek/deepseek-chat",
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": "Você é um jornalista profissional especializado em resumir notícias."},
            {"role": "user", "content": prompt_text}
        ],
        "temperature": 0.7
    }

    try:
        #print(payload)
        response = requests.post(API_URL, headers=HEADERS, json=payload)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        return f"Erro: {e}"

# 4. Processa todos os prompts
resumos = []
for idx, row in df.iterrows():
    print(f"Processando grupo {idx+1}/{len(df)} do tema {row['Tema']}...")

    prompt = row['Prompt']
    resumo = resumir_prompt(prompt)

    resumos.append({
        "Tema": row['Tema'],
        #"GrupoID": row['GrupoID'],
        #"QtdNoticias": row['QtdNoticias'],
        "Id": row['Ids'],
        "Resumo": resumo
    })

    time.sleep(2)  # pausa para respeitar limites da API

# 5. Salva resultados
df_resumo_setor = pd.DataFrame(resumos)
df_resumo_setor.to_excel(arq_results_setor, index=False)
print("Arquivo salvo: ", arq_results_setor)

"""#Chamada da API - **EDITORIAIS**"""

# Chamada da API para trazer os EDITORIAIS da MVC CommoditiesTemp J&F Eitoriais TOP4

# Caminho para o arquivo externo com as configurações da API
config_file = os.path.join(JF_path, "dados", "api_editorial_configs.json")  # Nome do arquivo de configuração

# Carrega as configurações da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configurações da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se não, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisição POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisição foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame à lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisição para {url}: {response.status_code}")

# Concatena todos os DataFrames em um único DataFrame
final_df_editorial = pd.concat(all_dfs, ignore_index=True)

# Variável com os títulos a ignorar
#titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "horóscopo", \
#                     "mensagens", "MIRIAM LEITÃO", "MÔNICA BERGAMO", "multitela", "Obituário", \
#                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
#                     "coluna do estadão", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_editorial['Titulo'] = final_df_editorial['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo começa com os termos a ignorar (comparação em minúsculas)
# Criar uma máscara booleana para as linhas a serem mantidas
#mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a máscara para remover as linhas indesejadas
#final_df_setor = final_df_setor[mask].copy()

# Grava o DataFrame final em um arquivo Excel

final_df_editorial.to_excel(arq_api_original_editorial, index=False)

# Criar o DataFrame 'final_df_small' após a filtragem
final_df_editorial_small = final_df_editorial[['Id', 'Titulo', 'Conteudo', 'IdVeiculo']].copy()


final_df_editorial_small.to_excel(arq_api_editorial, index=False)

print("Arquivo Excel salvo como ", arq_api_original_editorial)
print("Arquivo Excel salvo como ", arq_api_editorial)

"""#Chamada da API - **SPECIALS (Colunistas, Editoriais, 1a Página)**"""

# Chamada da API para trazer os Colunistas, Editoriais e 1a Página da MVC CommoditiesTemp J&F COL-EDT-CAPA TOP4

# Caminho para o arquivo externo com as configurações da API
config_file = os.path.join(JF_path, "dados", "api_SPECIALS_configs.json")  # Nome do arquivo de configuração

# Carrega as configurações da API do arquivo JSON
with open(config_file, "r") as f:
    api_configs = json.load(f)

# Lista para armazenar todos os DataFrames
all_dfs = []

# Itera sobre as configurações da API
for config in api_configs:
    url = config["url"]
    data = config["data"]

    # Verifica se a pasta 'api' existe, se não, cria
    api_path = os.path.join(JF_path, "api")
    if not os.path.exists(api_path):
        os.makedirs(api_path)
    # Envia a requisição POST para a API
    response = requests.post(url, json=data)

    # Verifique se a requisição foi bem-sucedida
    if response.status_code == 200:
        # Converte a resposta em JSON
        news_data = response.json()

        # Converte os dados em um DataFrame do pandas
        df_api = pd.json_normalize(news_data)

        # Adiciona o DataFrame à lista
        all_dfs.append(df_api)
    else:
        print(f"Erro na requisição para {url}: {response.status_code}")

# Concatena todos os DataFrames em um único DataFrame
final_df_SPECIALS = pd.concat(all_dfs, ignore_index=True)

# Variável com os títulos a ignorar
#titulos_a_ignorar = ["capa", "Alice Ferraz", "Curtas", "Editorial", "Expediente", "horóscopo", \
#                     "mensagens", "MIRIAM LEITÃO", "MÔNICA BERGAMO", "multitela", "Obituário", \
#                     "Outro canal", "Painel", "Play", "sesc", "cartas de leitores", "coluna de broadcast", \
#                     "coluna do estadão", "frase do dia"]

# Converter a coluna 'Titulo' para string e preencher NaNs com vazio para evitar erros
final_df_SPECIALS['Titulo'] = final_df_SPECIALS['Titulo'].astype(str).fillna('')

# Filtrar registros cujo Titulo começa com os termos a ignorar (comparação em minúsculas)
# Criar uma máscara booleana para as linhas a serem mantidas
#mask = ~final_df_setor['Titulo'].str.lower().str.startswith(tuple(t.lower() for t in titulos_a_ignorar))

# Aplicar a máscara para remover as linhas indesejadas
#final_df_setor = final_df_setor[mask].copy()

# Grava o DataFrame final em um arquivo Excel

final_df_SPECIALS.to_excel(arq_api_original_SPECIALS, index=False)


# Criar o DataFrame 'final_df_small' após a filtragem
final_df_SPECIALS_small = final_df_SPECIALS[['Id', 'Canais']].copy()


final_df_SPECIALS_small.to_excel(arq_api_SPECIALS, index=False)

print("Arquivo Excel salvo como ", arq_api_original_SPECIALS)
print("Arquivo Excel salvo como ", arq_api_SPECIALS)

"""#Geração do Resumo final"""

"""##Versão **Preliminar**"""


import pandas as pd
import pyshorteners
from docx import Document
from docx.shared import Pt  # Import Pt for point size
from docx.enum.text import WD_LINE_SPACING  # Import line spacing enum
from docx.shared import Inches
from docx.text.run import Run
from docx.enum.style import WD_STYLE_TYPE # Import WD_STYLE_TYPE
import re
import time # Importar time para pausa


# 1. Carregar os DataFrames
df_resumo_marca = pd.read_excel(arq_results_final) # Renomeado para clareza
final_df_marca = pd.read_excel(arq_api_original) # Renomeado para clareza
df_resumo_setor = pd.read_excel(arq_results_setor) # Carregar resultados do setor
final_df_setor = pd.read_excel(arq_api_original_setor) # Carregar dados originais do setor
final_df_editorial = pd.read_excel(arq_api_original_editorial) # Carregar dados originais dos editoriais
final_df_SPECIALS_small = pd.read_excel(arq_api_SPECIALS) # Carregar dados dos SPECIALS


# Load the final_df_small generated in the MARCA section
# We assume arq_api points to the small excel file from the MARCA section
try:
    final_df_small_marca = pd.read_excel(arq_api)
except FileNotFoundError:
    print(f"Erro: Arquivo {arq_api} não encontrado. Certifique-se de que a seção de MARCA foi executada.")
    # You might want to exit or handle this case differently
    final_df_small_marca = pd.DataFrame() # Create empty DataFrame to avoid further errors


# 2. Inicializar o documento DOCX
document = Document()

# Configurar o estilo Normal para remover espaço após o parágrafo e usar espaçamento simples
styles = document.styles
style = styles['Normal']
font = style.font
font.name = 'Calibri'  # Set the font to Calibri
style.paragraph_format.space_after = Pt(0)
style.paragraph_format.line_spacing_rule = WD_LINE_SPACING.SINGLE


# --- Seção Original para resumos de Marca ---
# 3. Iterar sobre df_resumo (Marca)
print(f"Processando {len(df_resumo_marca)} resumos de Marca...") # Debug print
for index, row_marca in df_resumo_marca.iterrows(): # Renomeado para clareza
    # Inicializar a string para cada grupo de notícias
    group_string = ""

    # 4. Iterar sobre os IDs das notícias (Marca)
    # Check if 'Ids' column exists and is not None
    if 'Ids' not in row_marca or pd.isna(row_marca['Ids']):
        print(f"Aviso: Linha {index} no df_resumo_marca não tem IDs válidos. Pulando.")
        continue

    for news_id_str in str(row_marca['Ids']).split(','): # Ensure it's a string
        try:
            news_id = int(news_id_str.strip())  # Convert to integer, strip whitespace
        except ValueError:
            print(f"Aviso: Não foi possível converter ID '{news_id_str}' para inteiro na linha {index} de df_resumo_marca. Pulando.")
            continue # Skip this ID if not a valid number


        # 5. Consultar informações no final_df_marca
        # Use final_df_marca for original info (like Veiculo, UrlVisualizacao, CanaisCommodities)
        news_info_marca = final_df_marca[final_df_marca['Id'] == news_id]
        if news_info_marca.empty:
             print(f"Aviso: ID {news_id} não encontrado em final_df_marca para resumo de Marca. Pulando.")
             continue # Skip this news if not found

        news_info_marca = news_info_marca.iloc[0]

        w_veiculo_marca = news_info_marca['Veiculo']
        w_url_marca = news_info_marca['UrlVisualizacao']
        #w_canais_commodities_marca = news_info_marca['CanaisCommodities'] # Obter CanaisCommodities (se disponível)


        # 6. Encurtar a URL
        s = pyshorteners.Shortener()
        retries = 3  # Number of retries
        short_url_marca = w_url_marca # Inicializa com a URL original
        for i in range(retries):
            try:
                short_url_marca = s.tinyurl.short(w_url_marca)
                break  # Exit the loop if successful
            except requests.exceptions.RequestException as e:
                # Check if the error is due to a redirect or other network issue
                if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                     print(f"Erro de conexão/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Marca ID {news_id}: {e}")
                else:
                    print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Marca ID {news_id}: {e}")

                if i < retries - 1:
                    time.sleep(2)  # Wait before retrying
                else:
                    print(f"Falha ao encurtar URL para Marca ID {news_id} após {retries} tentativas. Usando URL original.")
                    short_url_marca = w_url_marca # Use original URL if all retries fail


        # **Atualizar o DataFrame arq_api** (Atualiza final_df_small_marca)
        # Certifica-se de que a linha existe antes de tentar atualizar
        if news_id in final_df_small_marca['Id'].values:
             final_df_small_marca.loc[final_df_small_marca['Id'] == news_id, 'ShortURL'] = short_url_marca
        else:
             # This might happen if the small DF was filtered differently or not generated correctly
             print(f"Aviso: ID {news_id} não encontrado em final_df_small_marca para adicionar ShortURL. Ignorando atualização do ShortURL neste DF.")
             # Optionally, add the news info to final_df_small_marca if it's missing
             # new_row = news_info_marca.copy()
             # new_row['ShortURL'] = short_url_marca
             # final_df_small_marca = pd.concat([final_df_small_marca, pd.DataFrame([new_row])], ignore_index=True)


        # 7. Criar a string formatada

        # Incluir trecho de código para identificar o tipo "Special"
        special_type = ""
        # Busca o Id no final_df_SPECIALS_small
        special_info = final_df_SPECIALS_small[final_df_SPECIALS_small['Id'] == news_id]

        if not special_info.empty:
            # Se encontrar, verifica o campo Canais
            # Certifica-se de que 'Canais' é uma lista ou string e a processa
            canais_special = special_info.iloc[0]['Canais']
            if isinstance(canais_special, list):
                 canais_str = ', '.join(map(str, canais_special)) # Converte lista para string
            else:
                 canais_str = str(canais_special) # Garante que é string


            if "Editoriais" in canais_str:
                special_type = "Editorial"
            elif "Colunistas" in canais_str:
                special_type = "Colunista"
            elif "1ª Página" in canais_str:
                special_type = "Capa"
            # Se não contiver nenhuma das strings acima, special_type permanece ""

        # Formata a string incluindo o tipo "Special" se encontrado
        if special_type:
             group_string += f"{w_veiculo_marca} ({special_type} - {short_url_marca}), "
        else:
             group_string += f"{w_veiculo_marca} ({short_url_marca}), "


    # 8. Limpar e adicionar o resumo à string
    # Remover a última vírgula e espaço da string de veículos/urls
    group_string = group_string.rstrip(', ')
    group_string += "\n" # Adicionar quebra de linha antes do resumo

    # Check if 'Resumo' column exists and is not None
    if 'Resumo' not in row_marca or pd.isna(row_marca['Resumo']):
         print(f"Aviso: Linha {index} no df_resumo_marca não tem Resumo. Adicionando placeholder.")
         resumo_limpo = "[Resumo não disponível]"
    else:
        resumo_limpo = str(row_marca['Resumo']) # Ensure it's string first
        # Remove the specific string and strip whitespace - This is handled in the final processing step now
        # resumo_limpo = resumo_limpo.replace("(160 palavras)", "").strip()


    group_string += resumo_limpo

    # 9. Adicionar a string ao documento DOCX
    document.add_paragraph(group_string)
    document.add_paragraph("")  # Adicionar linha em branco


# DEBUG: Check final_df_small_marca before sorting
print(f"\nVerificando final_df_small_marca antes de ordenar para links:")
print(f"  Tem {len(final_df_small_marca)} linhas.")
print(f"  Tem coluna 'Canais'? {'Canais' in final_df_small_marca.columns}")

if not final_df_small_marca.empty and 'Canais' in final_df_small_marca.columns:
    # Sort the DataFrame
    # Assuming you still want the specific order defined by marca1, marca2, w_marcas
    # Ensure marca1, marca2, w_marcas are defined before this block
    # If not, a simple sort_values('Canais') will work but might not match your desired order
    try:
        # Attempt the custom sort if order is defined
        # Ensure marca1, marca2, w_marcas are accessible here (defined earlier in the notebook)
        order = [marca1, marca2] + [marca for marca in final_df_small_marca['Canais'].unique() if marca not in (marca1, marca2)]
        final_df_small_marca_sorted = final_df_small_marca.sort_values(by=['Canais'], key=lambda x: pd.Categorical(x, categories=order, ordered=True))
        print("Ordenação personalizada dos links por Marca aplicada.")
    except (NameError, KeyError, AttributeError) as e:
        print(f"Aviso: Falha na ordenação personalizada dos links por Marca ({type(e).__name__}: {e}). Verifique se marca1, marca2 e w_marcas estão definidos corretamente.")
        print("Ordenando por Canais padrão.")
        final_df_small_marca_sorted = final_df_small_marca.sort_values(by=['Canais'])


    # Adicionar uma quebra de linha entre a seção de Setor e a seção de links de Marca (se houver links de Marca)
    document.add_paragraph("")
    document.add_paragraph("--- Links das Notícias de Marca ---") # Opcional: um título para a seção de links
    document.add_paragraph("")

    current_marca = None  # Inicializar variável para controlar a marca atual

    for index, row_small_marca in final_df_small_marca_sorted.iterrows(): # Use the sorted DataFrame
        marca = row_small_marca['Canais']
        if marca != current_marca:  # Verificar se a marca mudou
            if current_marca is not None:  # Add blank line if not the first brand
                document.add_paragraph("")
            document.add_paragraph(f"*{marca}*")  # Incluir nome da marca
            current_marca = marca

        # Obter informações do final_df_marca (original dataframe) para Veiculo, Titulo, CanaisCommodities
        # Assumindo que final_df_marca e final_df_small_marca compartilham o mesmo 'Id'
        news_id_small = row_small_marca['Id']
        original_row_marca = final_df_marca[final_df_marca['Id'] == news_id_small]
        if original_row_marca.empty:
             print(f"Aviso: ID {news_id_small} não encontrado em final_df_marca para links de Marca. Pulando este link.")
             continue # Skip this link if original info not found

        original_row_marca = original_row_marca.iloc[0]

        veiculo = original_row_marca['Veiculo']
        titulo = original_row_marca['Titulo'] # Use titulo from original df
        # Verifique se 'CanaisCommodities' existe no seu final_df_marca
        canais_commodities = original_row_marca.get('CanaisCommodities', '') # Usa .get para evitar erro se a coluna não existir


        # Incluir linha com Veiculo e Titulo
        document.add_paragraph(f"{veiculo}: {titulo}") # Use 'titulo' variable

        # Incluir linha com ShortURL e Coluna (se aplicável)
        # Ensure 'ShortURL' column exists in final_df_small_marca
        short_url = row_small_marca.get('ShortURL', original_row_marca.get('UrlVisualizacao', 'URL Não Encontrada')) # Get ShortURL from small DF or fallback to original URL

        prefix = "Coluna - " if "Colunistas" in str(canais_commodities) else ""
        document.add_paragraph(f"{prefix}{short_url}")

        # Check if it's the last news item for the current brand
        # Check against the sorted DataFrame
        # Ensure we don't go out of bounds
        if index + 1 < len(final_df_small_marca_sorted):
             if final_df_small_marca_sorted.iloc[index + 1]['Canais'] == marca:
                # If not the last item, add the asterisk line
                document.add_paragraph("*")
        # If it is the last item for this brand, the next iteration will add a blank line if needed.
else:
    print("DataFrame 'final_df_small_marca' não encontrado, vazio ou sem a coluna 'Canais'. Pulando a seção de links por Marcas.")


# --- Novo Passo: Incluir resumos do Setor ---
# DEBUG: Check if df_resumo_setor is populated
print(f"\nVerificando df_resumo_setor:")
print(f"  Tem {len(df_resumo_setor)} linhas.")
print(f"  Está vazio? {df_resumo_setor.empty}")
if not df_resumo_setor.empty:
    document.add_paragraph("")
    document.add_paragraph("--- Notícias de Setor ---") # Opcional: um título para a seção de setor
    document.add_paragraph("")


# 10. Iterar o dataframe arq_results_setor (já deve estar ordenado por Tema)
current_tema = None  # Inicializar variável para controlar o tema atual

# Use df_resumo_setor, not arq_results_setor
for index, row_setor in df_resumo_setor.iterrows():
    # Check if 'Tema' column exists and is not None
    if 'Tema' not in row_setor or pd.isna(row_setor['Tema']):
        print(f"Aviso: Linha {index} no df_resumo_setor não tem Tema. Pulando.")
        continue

    tema = row_setor['Tema']

    # 11. Quando iniciar um Tema novo, incluir uma linha com o conteúdo do campo Tema, precedido e sucedido por um asterisco.
    if tema != current_tema:
        if current_tema is not None: # Adicionar linha em branco entre temas
            document.add_paragraph("")
        document.add_paragraph(f"*{tema}*")
        current_tema = tema

    # 12. Buscar informações no final_df_setor usando o Id
    # Check if 'Id' column exists and is not None
    if 'Id' not in row_setor or pd.isna(row_setor['Id']):
        print(f"Aviso: Linha {index} no df_resumo_setor não tem Id válido. Pulando.")
        continue

    try:
        news_id = int(str(row_setor['Id']).strip()) # Convert to integer, ensure string then strip
    except ValueError:
        print(f"Aviso: Não foi possível converter ID '{row_setor['Id']}' para inteiro na linha {index} de df_resumo_setor. Pulando.")
        continue # Skip this ID if not a valid number

    news_info_setor = final_df_setor[final_df_setor['Id'] == news_id]
    if news_info_setor.empty:
        print(f"Aviso: ID {news_id} não encontrado em final_df_setor para resumo de Setor. Pulando.")
        continue # Skip this news if not found
    news_info_setor = news_info_setor.iloc[0]

    w_veiculo_setor = news_info_setor['Veiculo']
    w_titulo_setor = news_info_setor['Titulo']
    w_url_setor = news_info_setor['UrlVisualizacao']

    # 13. Incluir linha com Veiculo e Titulo
    document.add_paragraph(f"{w_veiculo_setor}: {w_titulo_setor}")

    # 14. Incluir o Resumo do Setor
    # Check if 'Resumo' column exists and is not None
    if 'Resumo' not in row_setor or pd.isna(row_setor['Resumo']):
        print(f"Aviso: Linha {index} no df_resumo_setor (Tema {tema}) não tem Resumo. Adicionando placeholder.")
        document.add_paragraph("[Resumo não disponível]")
    else:
        resumo_bruto = str(row_setor['Resumo'])
        # Remove prefix like "**Resumo (90 palavras):**" or "**Resumo (160 palavras):**"
        # This pattern is now covered by the more general removal later.
        # Removing specific prefixes here might be redundant or interfere with the final step.
        # Keeping the raw resumo to be processed in the final cleanup loop.
        document.add_paragraph(resumo_bruto) # Add raw resumo, will be processed later


    # 15. Incluir a URL encurtada (Encapsular a lógica de encurtamento)
    s = pyshorteners.Shortener()
    retries = 3
    short_url_setor = w_url_setor # Inicializa com a URL original
    for i in range(retries):
        try:
            short_url_setor = s.tinyurl.short(w_url_setor)
            break
        except requests.exceptions.RequestException as e:
            # Check if the error is due to a redirect or other network issue
            if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                print(f"Erro de conexão/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Setor ID {news_id}: {e}")
            else:
                print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Setor ID {news_id}: {e}")
            if i < retries - 1:
                time.sleep(2)
            else:
                print(f"Falha ao encurtar URL para Setor ID {news_id} após {retries} tentativas. Usando URL original.")

    document.add_paragraph(short_url_setor)

    # 16. Incluir a linha com um asterisco
    document.add_paragraph("*")



# --- Novo Passo: Incluir Editoriais ---
print(f"\nProcessando {len(final_df_editorial)} editoriais...")
if not final_df_editorial.empty:
    print('Entrei aqui...')
    document.add_paragraph("")
    document.add_paragraph("*EDITORIAIS*")
    document.add_paragraph("") # Linha em branco após o título dos editoriais

    for index, row_editorial in final_df_editorial.iterrows():
        # 17. Incluir uma linha com o conteúdo do campo Veiculo, mais a string ": ", mais o conteúdo do campo Conteudo
        w_veiculo_editorial = row_editorial.get('Veiculo', 'Veículo Desconhecido')
        w_conteudo_editorial = row_editorial.get('Titulo', 'Título não disponível')
        document.add_paragraph(f"{w_veiculo_editorial}: {w_conteudo_editorial}")

        # 18. Incluir uma linha com a url encurtada do conteúdo do campo UrlVisualizacao
        w_url_editorial = row_editorial.get('UrlVisualizacao', 'URL Não Disponível')

        s = pyshorteners.Shortener()
        retries = 3
        short_url_editorial = w_url_editorial
        for i in range(retries):
            try:
                short_url_editorial = s.tinyurl.short(w_url_editorial)
                break
            except requests.exceptions.RequestException as e:
                if "Read timed out" in str(e) or "connection" in str(e) or "Max retries exceeded" in str(e):
                    print(f"Erro de conexão/timeout ao encurtar URL (tentativa {i+1}/{retries}) para Editorial {index}: {e}")
                else:
                    print(f"Erro ao encurtar URL (tentativa {i+1}/{retries}) para Editorial {index}: {e}")
                if i < retries - 1:
                    time.sleep(2)
                else:
                    print(f"Falha ao encurtar URL para Editorial {index} após {retries} tentativas. Usando URL original.")

        document.add_paragraph(short_url_editorial)
        document.add_paragraph("*") # Adicionar linha com asterisco após cada editorial



# ===== PROCESSAMENTO DO DOCUMENTO ANTES DA GRAVAÇÃO =====
# Este código deve ser inserido logo antes da linha: document.save(arq_resumo_final)

print("\nProcessando documento antes da gravação para remover padrões...")

paragraphs_to_remove_indices = []  # Lista de índices para remoção

# Pattern for lines starting with "*(" and ending with "palavras)"
pattern_line_start_paren_end_palavras = r"^\s*\*\s*\(.*?palavras\)\s*$"
# Pattern for lines starting with "*Resumo"
pattern_line_start_resumo = r"^\s*\*Resumo.*$"
# Pattern for any occurrence of "(*...palavras*)" or "(...palavras)" or "**Resumo (...):**" etc.
# Let's refine this to catch various formats
pattern_parenthesized_palavras_general = r"\s*[\*\s]*\(.*?\s*palavras\s*\)[\s\:\*]*"
# Add patterns for specific prefixes like "**Resumo:**" or "*Resumo:*"
pattern_specific_resumo_prefixes = r"^\s*[\*\s]*Resumo\s*[:\*\s]*"


for i, paragraph in enumerate(document.paragraphs):
    texto = paragraph.text

    # 1) Eliminar todas as linhas que começam com a string "*(" E terminam com a string "palavras)"
    if re.fullmatch(pattern_line_start_paren_end_palavras, texto, re.IGNORECASE):
        paragraphs_to_remove_indices.append(i)
        print(f"  Removendo linha {i} por corresponder ao padrão '* (... palavras)'")
        continue # Skip further processing for this paragraph

    # 2) Eliminar as linhas que começam com a string "*Resumo"
    if re.fullmatch(pattern_line_start_resumo, texto, re.IGNORECASE):
         paragraphs_to_remove_indices.append(i)
         print(f"  Removendo linha {i} por começar com '*Resumo'")
         continue # Skip further processing for this paragraph


    # 3) Eliminar as strings que começam com a string "*(", e que tenham algum conteúdo na sequência, e em seguida encerrem com a string "palavras)"
    # Also handle patterns like "**Resumo (...):**" and "*Resumo:*"
    new_text = re.sub(pattern_parenthesized_palavras_general, '', texto, flags=re.IGNORECASE).strip()
    new_text = re.sub(pattern_specific_resumo_prefixes, '', new_text, flags=re.IGNORECASE).strip()


    # If the text changed, update the paragraph
    if new_text != texto.strip(): # Compare with stripped original text
        paragraph.text = new_text
        print(f"  Removido padrão '(... palavras)' ou prefixos de resumo na linha {i}")

    # Replace any remaining "**" with "*"
    if "**" in paragraph.text:
         paragraph.text = paragraph.text.replace("**", "*")
         # print(f"  Substituído '**' por '*' na linha {i}") # Optional debug print


# Remover os parágrafos marcados para remoção (em ordem reversa)
for i in sorted(paragraphs_to_remove_indices, reverse=True):
    p = document.paragraphs[i]._element
    p.getparent().remove(p)

print(f"\nTotal de parágrafos removidos: {len(paragraphs_to_remove_indices)}")


# 3. Agora salvar o documento processado
document.save(arq_resumo_final)

# 19. Salvar o documento DOCX
# document.save(arq_resumo_final) # Já salvo acima
print(f"Arquivo DOCX salvo em: {arq_resumo_final}")

"""##Realce dos Hyperlinks - **Versão Ajustada** (source: Claude)"""

# Instalar as bibliotecas necessárias no Google Colab
##!pip install python-docx

import re
from docx import Document
from docx.shared import RGBColor
from docx.oxml.shared import OxmlElement, qn

def adicionar_hyperlink(paragraph, url, texto_display):
    """
    Adiciona um hyperlink a um parágrafo no documento Word
    """
    # Criar o elemento hyperlink
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), paragraph.part.relate_to(url,
                                                       "http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink",
                                                       is_external=True))

    # Criar o run com o texto do link
    new_run = OxmlElement('w:r')

    # Configurar propriedades do texto (cor azul, sublinhado)
    rPr = OxmlElement('w:rPr')

    # Cor azul
    color = OxmlElement('w:color')
    color.set(qn('w:val'), '0000FF')
    rPr.append(color)

    # Sublinhado
    underline = OxmlElement('w:u')
    underline.set(qn('w:val'), 'single')
    rPr.append(underline)

    new_run.append(rPr)

    # Adicionar o texto
    new_run.text = texto_display
    hyperlink.append(new_run)

    return hyperlink

def processar_urls_em_paragrafo(paragraph):
    """
    Processa um parágrafo, convertendo URLs em hyperlinks
    """
    texto_completo = paragraph.text

    # REGEX CORRIGIDA - Captura URLs completas incluindo parâmetros, fragmentos, etc.
    # Esta regex é mais robusta e captura URLs até encontrar espaço ou fim de linha
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    # Alternativa ainda mais simples e eficaz:
    # padrao_url = r'https?://\S+'

    urls_encontradas = re.findall(padrao_url, texto_completo)

    # Remover URLs duplicadas mantendo a ordem
    urls_unicas = []
    for url in urls_encontradas:
        if url not in urls_unicas:
            urls_unicas.append(url)

    if urls_unicas:
        print(f"   🔗 Encontradas {len(urls_unicas)} URLs: {urls_unicas[:2]}{'...' if len(urls_unicas) > 2 else ''}")

        # Limpar o parágrafo atual
        paragraph.clear()

        # Dividir o texto pelas URLs
        texto_restante = texto_completo

        for url in urls_unicas:
            # Encontrar a posição da URL no texto
            if url in texto_restante:
                partes = texto_restante.split(url, 1)

                if len(partes) == 2:
                    # Adicionar texto antes da URL
                    if partes[0]:
                        paragraph.add_run(partes[0])

                    # Adicionar hyperlink
                    hyperlink_element = adicionar_hyperlink(paragraph, url, url)
                    paragraph._p.append(hyperlink_element)

                    # Continuar com o resto do texto
                    texto_restante = partes[1]

        # Adicionar texto restante após a última URL
        if texto_restante:
            paragraph.add_run(texto_restante)

        return True

    return False

def converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida):
    """
    Converte URLs em hyperlinks em um arquivo DOCX
    """
    try:
        # Abrir o documento
        print(f"📖 Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        urls_convertidas = 0
        paragrafos_processados = 0
        paragrafos_com_urls = 0

        # Processar todos os parágrafos
        print("🔄 Processando parágrafos...")
        for i, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():  # Ignorar parágrafos vazios
                if processar_urls_em_paragrafo(paragraph):
                    paragrafos_com_urls += 1
                paragrafos_processados += 1

                # Mostrar progresso a cada 25 parágrafos
                if paragrafos_processados % 25 == 0:
                    print(f"   📄 Processados {paragrafos_processados} parágrafos ({paragrafos_com_urls} com URLs)...")

        # Processar tabelas (se houver)
        print("📊 Processando tabelas...")
        tabelas_processadas = 0
        celulas_com_urls = 0

        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if paragraph.text.strip():
                            if processar_urls_em_paragrafo(paragraph):
                                celulas_com_urls += 1
            tabelas_processadas += 1

        # Salvar o documento processado
        print(f"💾 Salvando arquivo: {arquivo_saida}")
        doc.save(arquivo_saida)

        print(f"\n✅ Conversão concluída com sucesso!")
        print(f"📊 Estatísticas:")
        print(f"   - Parágrafos processados: {paragrafos_processados}")
        print(f"   - Parágrafos com URLs: {paragrafos_com_urls}")
        print(f"   - Tabelas processadas: {tabelas_processadas}")
        print(f"   - Células com URLs: {celulas_com_urls}")
        print(f"   - Arquivo salvo como: {arquivo_saida}")

        return True

    except FileNotFoundError:
        print(f"❌ Erro: Arquivo '{arquivo_entrada}' não encontrado!")
        print("Verifique se o arquivo existe no diretório atual.")
        return False

    except Exception as e:
        print(f"❌ Erro durante o processamento: {str(e)}")
        print(f"Detalhes do erro: {type(e).__name__}")
        return False

def metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida):
    """
    Método alternativo melhorado - substitui URLs por texto com formatação
    """
    try:
        print(f"📖 Método alternativo melhorado - Abrindo arquivo: {arquivo_entrada}")
        doc = Document(arquivo_entrada)

        total_urls_encontradas = 0
        paragrafos_processados = 0

        # Regex melhorada para capturar URLs completas
        padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

        # Processar parágrafos
        for paragraph in doc.paragraphs:
            if not paragraph.text.strip():
                continue

            texto_original = paragraph.text
            urls_no_texto = re.findall(padrao_url, texto_original)

            if urls_no_texto:
                print(f"   🔗 Parágrafo {paragrafos_processados + 1}: {len(urls_no_texto)} URLs encontradas")
                total_urls_encontradas += len(urls_no_texto)

                # Limpar o parágrafo
                paragraph.clear()

                # Reconstruir o parágrafo com formatação
                texto_restante = texto_original

                for url in urls_no_texto:
                    if url in texto_restante:
                        # Dividir o texto pela URL
                        partes = texto_restante.split(url, 1)
                        if len(partes) == 2:
                            # Adicionar texto antes da URL
                            if partes[0]:
                                paragraph.add_run(partes[0])

                            # Adicionar URL com formatação especial
                            run_url = paragraph.add_run(url)
                            run_url.font.color.rgb = RGBColor(0, 0, 255)  # Azul
                            run_url.underline = True

                            # Continuar com o resto
                            texto_restante = partes[1]

                # Adicionar texto restante
                if texto_restante:
                    paragraph.add_run(texto_restante)

            paragrafos_processados += 1

        # Processar tabelas também
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    for paragraph in cell.paragraphs:
                        if not paragraph.text.strip():
                            continue

                        texto_original = paragraph.text
                        urls_no_texto = re.findall(padrao_url, texto_original)

                        if urls_no_texto:
                            total_urls_encontradas += len(urls_no_texto)
                            paragraph.clear()

                            texto_restante = texto_original
                            for url in urls_no_texto:
                                if url in texto_restante:
                                    partes = texto_restante.split(url, 1)
                                    if len(partes) == 2:
                                        if partes[0]:
                                            paragraph.add_run(partes[0])

                                        run_url = paragraph.add_run(url)
                                        run_url.font.color.rgb = RGBColor(0, 0, 255)
                                        run_url.underline = True

                                        texto_restante = partes[1]

                            if texto_restante:
                                paragraph.add_run(texto_restante)

        # Salvar documento
        doc.save(arquivo_saida)

        print(f"\n✅ Método alternativo concluído!")
        print(f"📊 Estatísticas:")
        print(f"   - Parágrafos processados: {paragrafos_processados}")
        print(f"   - Total de URLs formatadas: {total_urls_encontradas}")
        print(f"💾 Arquivo salvo como: {arquivo_saida}")

        return True

    except Exception as e:
        print(f"❌ Erro no método alternativo: {str(e)}")
        return False

def testar_regex():
    """
    Função para testar a regex com URLs de exemplo
    """
    print("🧪 Testando regex com URLs de exemplo...")

    # URLs de teste
    urls_teste = [
        "https://tinyurl.com/2aymnjlf",
        "https://www.google.com/search?q=python",
        "http://example.com/path/to/file.html",
        "https://github.com/user/repo#readme",
        "https://site.com/page?param1=value1&param2=value2"
    ]

    # Regex melhorada
    padrao_url = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s<>"{}|\\^`\[\].,;:!?)]'

    for url in urls_teste:
        match = re.search(padrao_url, url)
        if match:
            print(f"   ✅ {url} -> Capturado: {match.group()}")
        else:
            print(f"   ❌ {url} -> Não capturado")

    print("\n" + "="*50)

# Função principal para executar a conversão
def main():
    """
    Função principal para executar a conversão
    """
    # Nomes dos arquivos (ajuste conforme necessário)
    arquivo_entrada = arq_resumo_final  # Seu arquivo original
    arquivo_saida = arq_resumo_final_ajustado  # Arquivo com hyperlinks

    print("🚀 Iniciando conversão de URLs para hyperlinks...")
    print("=" * 60)

    # Testar regex primeiro
    testar_regex()

    # Tentar primeiro método (hyperlinks verdadeiros)
    print("\n🔗 Tentando método com hyperlinks verdadeiros...")
    sucesso = converter_urls_docx_para_hyperlinks(arquivo_entrada, arquivo_saida)

    if not sucesso:
        print("\n🔄 Tentando método alternativo melhorado...")
        print("-" * 50)
        arquivo_saida_alt = "arq_resumo_final_formatado.docx"
        sucesso = metodo_alternativo_melhorado(arquivo_entrada, arquivo_saida_alt)

    if sucesso:
        print("\n🎉 Processo finalizado com sucesso!")
        print("\n💡 Dicas:")
        print("   - Abra o arquivo no Word para verificar os hyperlinks")
        print("   - Os links devem estar em azul e sublinhados")
        print("   - Teste alguns links clicando neles")
        print("   - Se algum link ainda estiver truncado, execute novamente")
    else:
        print("\n❌ Não foi possível completar a conversão.")
        print("Verifique se o arquivo de entrada existe e não está corrompido.")

# Função de utilidade para listar arquivos DOCX no diretório atual
def listar_arquivos_docx():
    """
    Lista todos os arquivos .docx no diretório atual
    """
    import os
    arquivos_docx = [f for f in os.listdir('.') if f.endswith('.docx')]

    if arquivos_docx:
        print("📁 Arquivos .docx encontrados:")
        for i, arquivo in enumerate(arquivos_docx, 1):
            print(f"   {i}. {arquivo}")
    else:
        print("❌ Nenhum arquivo .docx encontrado no diretório atual.")

    return arquivos_docx

# Executar a conversão
if __name__ == "__main__":
    # Uncomment para listar arquivos disponíveis primeiro
    # listar_arquivos_docx()
    main()

"""#Final do Cronômetro"""

# Fim do Cronômetro e Cálculo do Tempo
end_time = datetime.datetime.now()
print(f"Fim do processamento: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

time_diff = end_time - start_time
minutes_spent = time_diff.total_seconds() / 60

print(f"Tempo gasto no processamento: {minutes_spent:.2f} minutos")